{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT5BL9nf73Yu",
        "outputId": "f0396722-8b21-4843-d219-c3611c363660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-storage-blob in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (12.25.1)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: pymilvus in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.5.10)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (1.34.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (44.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (4.11.0)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (0.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (1.26.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: setuptools>69 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (6.31.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (2.4.12)\n",
            "Requirement already satisfied: six>=1.11.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-core>=1.30.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: tqdm in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
            "Requirement already satisfied: pycparser in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/_m/dy6l5nmj1h986_4zdymz20240000gn/T/pip-req-build-eoa53g3z\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/_m/dy6l5nmj1h986_4zdymz20240000gn/T/pip-req-build-eoa53g3z\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: ftfy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (2.2.2)\n",
            "Requirement already satisfied: torchvision in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (0.17.2)\n",
            "Requirement already satisfied: wcwidth in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (2025.5.1)\n",
            "Requirement already satisfied: numpy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torchvision->clip==1.0) (1.26.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: ftfy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from ftfy) (0.2.5)\n",
            "Requirement already satisfied: torch in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (0.17.2)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: numpy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torchvision) (1.26.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install azure-storage-blob requests pandas pymilvus\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install torch torchvision  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgO-3_DDtKM-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import hashlib\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import requests as req\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, explode, col, array, lit, transform\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from hdfs import InsecureClient as HdfsClient\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import findspark\n",
        "from sqlalchemy import create_engine, inspect, text\n",
        "import io\n",
        "from PIL import Image\n",
        "import torch\n",
        "import clip\n",
        "from pymilvus import (\n",
        "    connections,\n",
        "    FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"AZURE_CONNECTION_STRING\"] = \"DefaultEndpointsProtocol=https;AccountName=bdmprojectaccount;AccountKey=B3D88p/oqHsgj58j5cI+I8bLpGnSFeUMgfVGFtCxeC8wHQKv1DDnQWGzy1/a5fMGnxmE3eYFqyji+AStJ7v2Bg==;EndpointSuffix=core.windows.net\"\n",
        "os.environ[\"RAPID_API_KEY\"] = \"356373667dmsh76bcf16aecc7050p1c4df5jsnffa784d3b2a0\"\n",
        "os.environ[\"RAPID_API_HOST\"] = \"booking-com15.p.rapidapi.com\"\n",
        "os.environ[\"OPENROUTE_KEY\"] = '5b3ce3597851110001cf624802e5afc167054c88ad92e9a69206707c'\n",
        "os.environ[\"AZURE_SAS_TOKEN\"] = 'sp=r&st=2025-05-14T15:15:23Z&se=2025-07-14T23:15:23Z&spr=https&sv=2024-11-04&sr=c&sig=Jekfw3ghnENVpQeK5e0yG8UoFivdrUNW%2BfGG8p%2FxfPQ%3D'\n",
        "storage_account_name = 'bdmprojectaccount'\n",
        "storage_container_name = 'bdmcontainerp1'\n",
        "os.environ[\"SPARK_HOME\"] = \"/Users/sebastianneri/spark/spark-3.5.3-bin-hadoop3\"\n",
        "JDBC_JAR = \"/Users/sebastianneri/drivers/postgresql-42.3.6.jar\"\n",
        "POSTGRES_URL = \"postgresql://sebas:mypassword@localhost:5432/exploitation_zone\" # Url of postgres\n",
        "POSTGRES_DRIVER_URL = \"jdbc:postgresql://localhost:5432/exploitation_zone\" # Driver to connect Spark with Postgres\n",
        "os.environ[\"SPARK_LOCAL_IP\"] = \"192.168.1.36\"\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BDMProject\") \\\n",
        "    .master(\"spark://localhost:7077\")\\\n",
        "    .config(\"spark.jars\", JDBC_JAR) \\\n",
        "    .config(\"spark.driver.extraClassPath\", JDBC_JAR) \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZQOLKA6O788v"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/28 21:00:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "connection_string = os.getenv('AZURE_CONNECTION_STRING')\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BDMProject\") \\\n",
        "    .master(\"spark://localhost:7077\")\\\n",
        "    .config(\"spark.jars\", JDBC_JAR) \\\n",
        "    .config(\"spark.driver.extraClassPath\", JDBC_JAR) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"OFF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHQMnHfmq29F"
      },
      "source": [
        "# Transportation\n",
        "\n",
        "Find out more in: https://openrouteservice.org/dev/#/api-docs/optimization/post\n",
        "Alternatives: https://github.com/graphhopper/graphhopper/blob/master/README.md#Map-Matching\n",
        "https://github.com/VROOM-Project/vroom/blob/master/docs/API.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def test_spark_connection() -> bool:\n",
        "    \"\"\"\n",
        "    Verifies the Spark session is active by performing a simple operation.\n",
        "    Returns True if the session is responsive, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Simple operation: create a small DataFrame and collect\n",
        "        count = spark.range(0, 1).count()\n",
        "        print (count == 1)\n",
        "    except Exception:\n",
        "        return print(False)\n",
        "\n",
        "# Run test\n",
        "if __name__ == '__main__':\n",
        "    test_spark_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_struct(flat_schema: dict) -> StructType:\n",
        "    fields = []\n",
        "    for name, typ in flat_schema.items():\n",
        "        if isinstance(typ, list):\n",
        "            elem = typ[0]\n",
        "            if isinstance(elem, dict):\n",
        "                struct = build_struct(elem)\n",
        "                fields.append(StructField(name, ArrayType(struct), True))\n",
        "            else:\n",
        "                spark_type = {'int': IntegerType(), 'float': DoubleType(), 'str': StringType(), 'bool': BooleanType()}.get(elem, StringType())\n",
        "                fields.append(StructField(name, ArrayType(spark_type), True))\n",
        "        else:\n",
        "            spark_type = {'int': IntegerType(), 'float': DoubleType(), 'str': StringType(), 'bool': BooleanType()}.get(typ, StringType())\n",
        "            fields.append(StructField(name, spark_type, True))\n",
        "    return StructType(fields)\n",
        "\n",
        "def string_to_sha256(text: str) -> str:\n",
        "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
        "\n",
        "def flatten_df(df):\n",
        "    from pyspark.sql.types import StructType\n",
        "    flat_cols = []\n",
        "    nested_cols = []\n",
        "    for field in df.schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            nested_cols.append(field.name)\n",
        "        else:\n",
        "            flat_cols.append(col(field.name))\n",
        "    for nested in nested_cols:\n",
        "        for f in df.schema[nested].dataType.fields:\n",
        "            flat_cols.append(col(f\"{nested}.{f.name}\").alias(f\"{nested}_{f.name}\"))\n",
        "    return df.select(flat_cols)\n",
        "\n",
        "# Configuración del Data Lake\n",
        "# Asume que 'file_system_name' es el filesystem de Delta Lake\n",
        "def get_data_lake_service(account_url: str, credential) -> DataLakeServiceClient:\n",
        "    return DataLakeServiceClient(account_url=account_url, credential=credential)\n",
        "\n",
        "@udf(StringType())\n",
        "def sha256_udf(url: str) -> str:\n",
        "    return hashlib.sha256(url.encode('utf-8')).hexdigest() if url else None\n",
        "\n",
        "@udf(StringType())\n",
        "def standardized_hours_udf(ts: str) -> str:\n",
        "    return datetime.fromisoformat(ts).strftime('%H:%M') if ts else None\n",
        "\n",
        "\n",
        "def file_exists(fs_client, path: str) -> bool:\n",
        "    try:\n",
        "        file_client = fs_client.get_file_client(path)\n",
        "        return file_client.exists()\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def upload_file(fs_client, path: str, data: bytes, overwrite=True) -> None:\n",
        "    file_client = fs_client.get_file_client(path)\n",
        "    if overwrite and file_client.exists():\n",
        "        file_client.delete_file()\n",
        "    file_client.create_file()\n",
        "    file_client.append_data(data, offset=0)\n",
        "    file_client.flush_data(len(data))\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# FLATTEN, CAST, SCHEMA Y COMPRESIÓN DE IMÁGENES (sin cambios)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def load_json_schema(path: str) -> dict:\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def compress_image(image_bytes: bytes, max_width: int = 1024, quality: int = 75) -> bytes:\n",
        "    with Image.open(BytesIO(image_bytes)) as img:\n",
        "        if img.mode in (\"RGBA\", \"P\"):\n",
        "            img = img.convert(\"RGB\")\n",
        "        if img.width > max_width:\n",
        "            ratio = max_width / float(img.width)\n",
        "            new_height = int(img.height * ratio)\n",
        "            img = img.resize((max_width, new_height), Image.LANCZOS)\n",
        "        buffer = BytesIO()\n",
        "        img.save(buffer, format=\"JPEG\", quality=quality, optimize=True)\n",
        "        return buffer.getvalue()\n",
        "\n",
        "def process_accommodation_record(record: dict, schema: dict) -> dict:\n",
        "    # Crear DF inferido\n",
        "    accommodation_schema_dict = json.loads(open('accommodation_schema.json').read())\n",
        "    accommodation_struct = build_struct(accommodation_schema_dict)\n",
        "     # Crear DF completo e inferido\n",
        "    df = spark.read.json(spark.sparkContext.parallelize([json.dumps(record)]))\n",
        "\n",
        "    # Extraer columnas planas y hasta 3 niveles de anidación\n",
        "\n",
        "    all_cols = []\n",
        "    for field in df.schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            for subfield in field.dataType.fields:\n",
        "                if isinstance(subfield.dataType, StructType):\n",
        "                    for subsubfield in subfield.dataType.fields:\n",
        "                        if isinstance(subsubfield.dataType, StructType):\n",
        "                            for subsubsubfield in subsubfield.dataType.fields:\n",
        "                                all_cols.append(\n",
        "                                    col(f\"{field.name}.{subfield.name}.{subsubfield.name}.{subsubsubfield.name}\").alias(f\"{field.name}_{subfield.name}_{subsubfield.name}_{subsubsubfield.name}\")\n",
        "                                )\n",
        "                        else:\n",
        "                            all_cols.append(\n",
        "                                col(f\"{field.name}.{subfield.name}.{subsubfield.name}\").alias(f\"{field.name}_{subfield.name}_{subsubfield.name}\")\n",
        "                            )\n",
        "                else:\n",
        "                    all_cols.append(\n",
        "                        col(f\"{field.name}.{subfield.name}\").alias(f\"{field.name}_{subfield.name}\")\n",
        "                    )\n",
        "        else:\n",
        "            all_cols.append(col(field.name))\n",
        "\n",
        "    df_flat = df.select(*all_cols)\n",
        "\n",
        "    # Generar hashes de fotos en Python\n",
        "    photo_urls = record.get(\"property\", {}).get(\"photoUrls\", [])\n",
        "    photo_hashes = [string_to_sha256(u) for u in photo_urls]\n",
        "    df_final = df_flat.withColumn(\"property_photoHash\", lit(photo_hashes))\n",
        "\n",
        "    # Serializar y validar con esquema oficial\n",
        "    json_flat = df_final.toJSON().first()\n",
        "    df_valid = spark.read.schema(accommodation_struct).json(spark.sparkContext.parallelize([json_flat]))\n",
        "    return df_valid.collect()[0].asDict()\n",
        "\n",
        "# --- Procesar registro de clima con Spark flatten y luego aplicar esquema ---\n",
        "def process_weather_record(raw: dict, schema: dict) -> dict:\n",
        "    weather_schema_dict = json.loads(open('weather_schema.json').read())\n",
        "    weather_struct = build_struct(weather_schema_dict)\n",
        "    raw_hourly = raw['hourly']\n",
        "    raw_hourly['timestamp'] = raw_hourly['time']\n",
        "    raw_hourly['time'] = [datetime.fromisoformat(t).strftime('%H:%M') for t in raw_hourly['time']]\n",
        "    \n",
        "    data_rows = [dict(zip(raw_hourly.keys(), values)) for values in zip(*raw_hourly.values())]\n",
        "\n",
        "    # 4. Crear el DataFrame con esquema aplicado\n",
        "    df_valid = spark.createDataFrame(data_rows, schema=weather_struct)\n",
        "    all_rows = [row.asDict() for row in df_valid.collect()]\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "def process_accommodation_images(photo_urls: list, fs_client, city: str) -> None:\n",
        "    for url in photo_urls:\n",
        "        sha = string_to_sha256(url)\n",
        "        path = f\"landing_zone/accommodation_images/{city}/{sha}.jpg\"\n",
        "        trusted_path = f\"trusted_zone/accommodation_images/{city}/{sha}.jpg\"\n",
        "        if file_exists(fs_client, path) and file_exists(fs_client, trusted_path):\n",
        "            continue\n",
        "        if file_exists(fs_client, path):\n",
        "            data = fs_client.get_file_client(path).download_file().readall()\n",
        "        else:\n",
        "            res = req.get(url, stream=True)\n",
        "            try:\n",
        "                res.raise_for_status()\n",
        "            except:\n",
        "                continue\n",
        "            data = res.content\n",
        "            upload_file(fs_client, path, data)\n",
        "        compressed = compress_image(data, max_width=800, quality=70)\n",
        "        upload_file(fs_client, trusted_path, compressed)\n",
        "\n",
        "BACKFILL_LOG = \"/exploitation_zone/backfilled_files.txt\"\n",
        "\n",
        "\n",
        "\n",
        "def init_clip(model_name: str = \"RN50\", device: str = None):\n",
        "    device = \"cpu\"\n",
        "    model, preprocess = clip.load(model_name, device=device)\n",
        "    model.eval()\n",
        "    return model, preprocess, device\n",
        "\n",
        "\n",
        "def image_bytes_to_embedding(\n",
        "    image_bytes: bytes,\n",
        "    model,\n",
        "    preprocess,\n",
        "    device,\n",
        "    normalize: bool = True\n",
        ") -> list:\n",
        "    img = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    inp = preprocess(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = model.encode_image(inp)\n",
        "    if normalize:\n",
        "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "    return emb.squeeze(0).cpu().tolist()\n",
        "\n",
        "\n",
        "def get_or_create_collection(\n",
        "    collection_name: str = \"accommodation_images\",\n",
        "    dim: int = 1024,\n",
        "    index_params: dict = None\n",
        ") -> Collection:\n",
        "\n",
        "    \n",
        "    # 3.2 Define schema if needed\n",
        "    if not utility.has_collection(collection_name):\n",
        "        fields = [\n",
        "            FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n",
        "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
        "        ]\n",
        "        schema = CollectionSchema(fields, description=f\"{collection_name} image embeddings\")\n",
        "        collection = Collection(name=collection_name, schema=schema)\n",
        "        # create index\n",
        "        idx = index_params or {\n",
        "            \"index_type\": \"IVF_FLAT\",\n",
        "            \"params\": {\"nlist\": 128},\n",
        "            \"metric_type\": \"L2\"\n",
        "        }\n",
        "        collection.create_index(field_name=\"embedding\", index_params=idx)\n",
        "    else:\n",
        "        collection = Collection(collection_name)\n",
        "    \n",
        "    # 3.3 Load into memory for speed\n",
        "    collection.load()\n",
        "    return collection\n",
        "\n",
        "\n",
        "def insert_embedding(\n",
        "    collection: Collection,\n",
        "    id: int,\n",
        "    vector: list\n",
        ") -> None:\n",
        "    collection.insert([[id], [vector]])\n",
        "\n",
        "\n",
        "def query_embeddings(\n",
        "    collection: Collection,\n",
        "    expr: str = \"\",\n",
        "    output_fields: list = None,\n",
        "    limit: int = 100\n",
        ") -> list:\n",
        "    output_fields = output_fields or [\"id\", \"embedding\"]\n",
        "    results = collection.query(expr=expr, output_fields=output_fields, limit=limit)\n",
        "    return results\n",
        "\n",
        "def _load_backfill_log() -> set:\n",
        "    \"\"\"\n",
        "    Ensure the backfill_log table exists, then load all file_key values into a set.\n",
        "    \"\"\"\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS backfill_log (\n",
        "                file_key TEXT PRIMARY KEY,\n",
        "                inserted_at TIMESTAMP NOT NULL\n",
        "            )\n",
        "        \"\"\"))\n",
        "        rows = conn.execute(text(\"SELECT file_key FROM backfill_log\")).all()\n",
        "    return {r.file_key for r in rows}\n",
        "\n",
        "\n",
        "\n",
        "def _append_to_backfill_log(log_key: str) -> None:\n",
        "    \"\"\"\n",
        "    Insert a new entry into backfill_log (or do nothing if already present).\n",
        "    \"\"\"\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            INSERT INTO backfill_log (file_key, inserted_at)\n",
        "            VALUES (:file_key, :inserted_at)\n",
        "            ON CONFLICT (file_key) DO NOTHING\n",
        "        \"\"\"), {\n",
        "            \"file_key\": log_key,\n",
        "            \"inserted_at\": datetime.utcnow()\n",
        "        })\n",
        "\n",
        "def _write_to_postgres(df, table_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Write a Spark DataFrame to Postgres, creating the table if it doesn't exist.\n",
        "    If the table is created, the schema and a sample of the data are shown.\n",
        "    After creation, the function confirms the table exists before writing.\n",
        "    Raises an error if table creation failed.\n",
        "    \"\"\"\n",
        "    inspector = inspect(engine)\n",
        "\n",
        "    if not inspector.has_table(table_name):\n",
        "        print(f\"🛠️ Table '{table_name}' does not exist. Creating it...\")\n",
        "\n",
        "        try:\n",
        "            # Create the table (schema only)\n",
        "            df.limit(0) \\\n",
        "              .write \\\n",
        "              .format(\"jdbc\") \\\n",
        "              .option(\"url\", POSTGRES_DRIVER_URL) \\\n",
        "              .option(\"dbtable\", table_name) \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .save()\n",
        "\n",
        "            # Confirm creation\n",
        "            inspector = inspect(engine)\n",
        "            if not inspector.has_table(table_name):\n",
        "                raise RuntimeError(f\"❌ Failed to create table '{table_name}'.\")\n",
        "\n",
        "            print(\"✅ Table created. Schema:\")\n",
        "            print(\"📊 Sample rows:\")\n",
        "            df.show(5, truncate=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(df.show())\n",
        "            raise RuntimeError(f\"❌ Error creating table '{table_name}': {e}\")\n",
        "\n",
        "    # Append data\n",
        "    try:\n",
        "        df.write \\\n",
        "          .format(\"jdbc\") \\\n",
        "          .option(\"url\", POSTGRES_DRIVER_URL) \\\n",
        "          .option(\"dbtable\", table_name) \\\n",
        "          .mode(\"append\") \\\n",
        "          .save()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(df.show())\n",
        "        raise RuntimeError(f\"❌ Error writing to table '{table_name}': {e}\")\n",
        "\n",
        "\n",
        "def _fetch_files(coll: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Connects to an existing Milvus collection and returns a list of all primary-key IDs.\n",
        "    \"\"\"\n",
        "    # How many entities are stored?\n",
        "    total = coll.num_entities\n",
        "    if total == 0:\n",
        "        return []\n",
        "    # Query for all IDs (no filter, just the 'id' field)\n",
        "    results = coll.query(\n",
        "        expr=\"\", \n",
        "        output_fields=[\"id\"], \n",
        "        limit=total\n",
        "    )\n",
        "    # Extract and return the id values\n",
        "    return [row[\"id\"] for row in results]\n",
        "\n",
        "\n",
        "\n",
        "def get_and_sync_accommodation(\n",
        "    fs,\n",
        "    spark: SparkSession,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    cities: dict,\n",
        "    query_template: dict,\n",
        "    headers:dict,\n",
        "    schema_file: str\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    delta = timedelta(days=1)\n",
        "    log_entries = _load_backfill_log()\n",
        "    coll = get_or_create_collection()\n",
        "    vector_ids = _fetch_files(coll)\n",
        "    model, preprocess, device = init_clip(\"RN50\")\n",
        "    for single_date in tqdm([start + i * delta for i in range((end - start).days + 1)]):\n",
        "        arrival = single_date.strftime('%Y-%m-%d')\n",
        "        departure = (single_date + delta).strftime('%Y-%m-%d')\n",
        "        for city, dest_id in tqdm(cities.items()):\n",
        "            landing_path = f\"landing_zone/accommodation/{city}/{arrival}_{departure}.json\"\n",
        "            trusted_path = f\"trusted_zone/accommodation/{city}/{arrival}_{departure}.json\"\n",
        "            \n",
        "            log_key = f\"accommodation/{city}/{arrival}_{departure}.json\"\n",
        "            # filtro: si existe en landing y trusted, nada que hacer\n",
        "            if file_exists(fs, landing_path) and file_exists(fs, trusted_path) and log_key in log_entries and False:\n",
        "                continue\n",
        "\n",
        "            # obtener JSON: de landing o API\n",
        "            if file_exists(fs, landing_path):\n",
        "                raw = fs.get_file_client(landing_path).download_file().readall().decode('utf-8')\n",
        "                data = json.loads(raw)\n",
        "            else:\n",
        "                params = dict(query_template, dest_id=dest_id, arrival_date=arrival, departure_date=departure)\n",
        "                res = req.get(accommodation_endpoint, params=params, headers=headers)\n",
        "                time.sleep(10)\n",
        "                res.raise_for_status()\n",
        "                data = res.json()\n",
        "                upload_file(fs, landing_path, json.dumps(data).encode('utf-8'))\n",
        "            \n",
        "\n",
        "            if file_exists(fs, trusted_path):\n",
        "                trusted = fs.get_file_client(trusted_path).download_file().readall().decode('utf-8')\n",
        "                docs = json.loads(trusted)\n",
        "                photo_hashes = [doc['property_photoHash'] for doc in docs]\n",
        "                photo_urls = [u for h in data['data']['hotels'] for u in h['property']['photoUrls']]\n",
        "\n",
        "            else:\n",
        "                photo_urls = [u for h in data['data']['hotels'] for u in h['property']['photoUrls']]\n",
        "                process_accommodation_images(photo_urls, fs, city)\n",
        "                docs = [process_accommodation_record(r, schema) for r in data['data']['hotels']]\n",
        "                photo_hashes = [doc['property_photoHash'] for doc in docs]\n",
        "                upload_file(fs, trusted_path, json.dumps(docs).encode('utf-8'))\n",
        "            \n",
        "            for hashes, urls in zip(photo_hashes, photo_urls):\n",
        "                    for hash, url in zip(hashes, urls):\n",
        "                        img_path = f\"trusted_zone/accommodation_images/{city}/{hash}.jpg\"\n",
        "                        if not file_exists(fs, img_path):\n",
        "                            process_accommodation_images([url], fs, city)\n",
        "                            data_bytes = fs.get_file_client(img_path).download_file().readall()\n",
        "                            with engine.begin() as conn:\n",
        "                                conn.execute(text(\"\"\"\n",
        "                                    INSERT INTO accommodation_images (id, image_bytes)\n",
        "                                    VALUES (:id, :img)\n",
        "                                    ON CONFLICT (id) DO NOTHING\n",
        "                                \"\"\"), {\"id\": hash, \"img\": data_bytes})\n",
        "                        \n",
        "                        if hash not in vector_ids:\n",
        "                            data_bytes = fs.get_file_client(img_path).download_file().readall()\n",
        "                            print(\"Generating embeddings...\")\n",
        "                            emb = image_bytes_to_embedding(data_bytes, model, preprocess, device)\n",
        "                            coll = get_or_create_collection(collection_name=\"accommodation_images\", dim=len(emb))\n",
        "                            print(\"Inserting embeddings...\")\n",
        "                            insert_embedding(coll, id=hash, vector=emb)\n",
        "                            _append_to_backfill_log(img_path)\n",
        "\n",
        "            exploitation_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(d) for d in docs]))\n",
        "            exploitation_df = exploitation_df.drop('property_priceBreakdown_benefitBadges')\n",
        "            exploitation_df = exploitation_df.withColumn(\"property_priceBreakdown_excludedPrice_value\",exploitation_df[\"property_priceBreakdown_excludedPrice_value\"].cast(DoubleType()))\n",
        "            exploitation_df = exploitation_df.withColumn(\"city_code_name\", lit(city))\n",
        "            _write_to_postgres(exploitation_df, table_name='accommodation')\n",
        "            _append_to_backfill_log(log_key)\n",
        "            \n",
        "\n",
        "            \n",
        "\n",
        "def get_and_sync_weather(\n",
        "    fs,\n",
        "    spark: SparkSession,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    coords: dict,\n",
        "    query_template: dict,\n",
        "    schema_file: str\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    delta = timedelta(days=1)\n",
        "    log_entries = _load_backfill_log()\n",
        "    \n",
        "    for single_date in tqdm([start + i * delta for i in range((end - start).days + 1)]):\n",
        "        prev_start = (single_date - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "        for city, coord in coords.items():\n",
        "            landing_path = f\"landing_zone/weather/{city}/{prev_start}.json\"\n",
        "            trusted_path = f\"trusted_zone/weather/{city}/{prev_start}.json\"\n",
        "            log_key = f\"weather/{city}/{prev_start}.json\"\n",
        "\n",
        "            # filtro: si existe en landing y trusted, nada que hacer\n",
        "            if file_exists(fs, landing_path) and file_exists(fs, trusted_path) and log_key in log_entries:\n",
        "                continue\n",
        "\n",
        "            # obtener JSON: de landing o API\n",
        "            if file_exists(fs, landing_path):\n",
        "                raw = fs.get_file_client(landing_path).download_file().readall().decode('utf-8')\n",
        "                data = json.loads(raw)\n",
        "            else:\n",
        "                params = dict(query_template, latitude=coord['latitude'], longitude=coord['longitude'], start_date=prev_start, end_date=prev_start)\n",
        "                res = req.get(weather_endpoint, params=params)\n",
        "                time.sleep(10)\n",
        "                res.raise_for_status()\n",
        "                data = res.json()\n",
        "                upload_file(fs, landing_path, json.dumps(data).encode('utf-8'))\n",
        "\n",
        "            if file_exists(fs, trusted_path):\n",
        "                trusted = fs.get_file_client(trusted_path).download_file().readall().decode('utf-8')\n",
        "                data_rows = json.loads(trusted)\n",
        "                weather_schema_dict = json.loads(open('weather_schema.json').read())\n",
        "                weather_struct = build_struct(weather_schema_dict)\n",
        "                df_valid = spark.createDataFrame(data_rows, schema=weather_struct)\n",
        "                doc = [row.asDict() for row in df_valid.collect()]\n",
        "            else:\n",
        "                doc = process_weather_record(data, schema)\n",
        "                upload_file(fs, trusted_path, json.dumps(doc).encode('utf-8'))\n",
        "            \n",
        "\n",
        "            exploitation_df = spark.read.json(spark.sparkContext.parallelize(doc))\n",
        "            exploitation_df = exploitation_df.withColumn(\"city_code_name\", lit(city))\n",
        "            _write_to_postgres(exploitation_df, table_name='weather')\n",
        "            _append_to_backfill_log(log_key)\n",
        "            \n",
        "\n",
        "def list_files(fs_client, prefix: str) -> list:\n",
        "    return [p.name for p in fs_client.get_paths(path=prefix) if not p.is_directory]\n",
        "\n",
        "\n",
        "def find_missing_blobs(fs, landing_prefix: str) -> list:\n",
        "    missing = []\n",
        "    for path in list_files(fs, 'landing_zone/' + landing_prefix):\n",
        "        trusted_path = f\"trusted_zone/{path.replace('landing_zone/', '', 1)}\"\n",
        "        if not file_exists(fs, trusted_path):\n",
        "            missing.append(path)\n",
        "    return missing\n",
        "\n",
        "\n",
        "def backfill_trusted_zone(\n",
        "    fs,\n",
        "    landing_prefix: str,\n",
        "    schema_file: str = None\n",
        ") -> None:\n",
        "    schema = load_json_schema(schema_file) if schema_file else None\n",
        "    to_fill = find_missing_blobs(fs, landing_prefix)\n",
        "\n",
        "    for landing_path in tqdm(to_fill):\n",
        "        trusted_path = f\"trusted_zone/{landing_path.replace('landing_zone/', '', 1)}\"\n",
        "        data_bytes = fs.get_file_client(landing_path).download_file().readall()\n",
        "\n",
        "        # imágenes de alojamiento\n",
        "        if landing_path.startswith('landing_zone/accommodation_images/'):\n",
        "            compressed = compress_image(data_bytes, max_width=800, quality=70)\n",
        "            upload_file(fs, trusted_path, compressed)\n",
        "            continue\n",
        "\n",
        "        # JSON de alojamiento\n",
        "        if landing_path.startswith('landing_zone/accommodation/'):\n",
        "            data = json.loads(data_bytes.decode('utf-8'))\n",
        "            docs = [process_accommodation_record(r, schema) for r in data['data']['hotels']]\n",
        "            upload_file(fs, trusted_path, json.dumps(docs).encode('utf-8'))\n",
        "            continue\n",
        "\n",
        "        # JSON de clima\n",
        "        if landing_path.startswith('landing_zone/weather/'):\n",
        "            data = json.loads(data_bytes.decode('utf-8'))\n",
        "            doc = process_weather_record(data, schema)\n",
        "            upload_file(fs, trusted_path, json.dumps(doc).encode('utf-8'))\n",
        "\n",
        "def backfill_exploitation_zone(\n",
        "    dl_client,\n",
        "    spark: SparkSession,\n",
        "    data_type: str,\n",
        "):\n",
        "    model, preprocess, device = init_clip(\"RN50\")\n",
        "    # Load log entries once\n",
        "    log_entries = _load_backfill_log()\n",
        "    coll = get_or_create_collection()\n",
        "    vector_ids = _fetch_files(coll)\n",
        "    # List all files under trusted_zone\n",
        "    all_paths = list_files(dl_client, \"trusted_zone/\")\n",
        "    # Filter relevant paths\n",
        "    qualified = []  # tuples of (full_path, city)\n",
        "    for path in all_paths:\n",
        "        parts = path.split('/')\n",
        "        d_type, city = parts[-3], parts[-2]\n",
        "        if d_type != data_type:\n",
        "            continue\n",
        "        full_path = f\"trusted_zone/{data_type}/{city}/{parts[-1]}\"\n",
        "        qualified.append((full_path, city))\n",
        "\n",
        "    if data_type in (\"accommodation\", \"weather\"):\n",
        "        for full_path, city in tqdm(qualified):\n",
        "            file_name = full_path.split('/')[-1]\n",
        "            log_key = f\"{data_type}/{city}/{file_name}\"\n",
        "            if log_key in log_entries:\n",
        "                continue\n",
        "\n",
        "            # read JSON from trusted\n",
        "            data_bytes = dl_client.get_file_client(full_path).download_file().readall()\n",
        "            docs = json.loads(data_bytes.decode('utf-8'))\n",
        "\n",
        "            # build an RDD of JSON strings (list for accommodation, single dict for weather)\n",
        "            if isinstance(docs, list):\n",
        "                rdd = spark.sparkContext.parallelize([json.dumps(d) for d in docs])\n",
        "            else:\n",
        "                rdd = spark.sparkContext.parallelize([json.dumps(docs)])\n",
        "\n",
        "            exploitation_df = spark.read.json(rdd)\n",
        "            if data_type == 'accommodation':\n",
        "                exploitation_df = exploitation_df.drop('property_priceBreakdown_benefitBadges')\n",
        "                exploitation_df = exploitation_df.withColumn(\"property_priceBreakdown_excludedPrice_value\",exploitation_df[\"property_priceBreakdown_excludedPrice_value\"].cast(DoubleType()))\n",
        "            exploitation_df = exploitation_df.withColumn(\"city_code_name\", lit(city))\n",
        "            _write_to_postgres(exploitation_df, table_name=data_type)\n",
        "            _append_to_backfill_log(log_key)\n",
        "\n",
        "\n",
        "    elif data_type == \"accommodation_images\":\n",
        "        # ensure images table\n",
        "        with engine.begin() as conn:\n",
        "            conn.execute(text(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS accommodation_images (\n",
        "                    id TEXT PRIMARY KEY,\n",
        "                    image_bytes BYTEA NOT NULL\n",
        "                )\n",
        "            \"\"\"))\n",
        "\n",
        "        for full_path, city in tqdm(qualified):\n",
        "            file_name = full_path.split('/')[-1]\n",
        "            image_id = os.path.splitext(file_name)[0]\n",
        "            log_key = f\"{data_type}/{city}/{file_name}\"\n",
        "            \n",
        "            if log_key not in log_entries:\n",
        "                data_bytes = dl_client.get_file_client(full_path).download_file().readall()\n",
        "                with engine.begin() as conn:\n",
        "                    conn.execute(text(\"\"\"\n",
        "                        INSERT INTO accommodation_images (id, image_bytes)\n",
        "                        VALUES (:id, :img)\n",
        "                        ON CONFLICT (id) DO NOTHING\n",
        "                    \"\"\"), {\"id\": image_id, \"img\": data_bytes})\n",
        "            \n",
        "            if file_name not in vector_ids:\n",
        "                data_bytes = dl_client.get_file_client(full_path).download_file().readall()\n",
        "                emb = image_bytes_to_embedding(data_bytes, model, preprocess, device)\n",
        "                coll = get_or_create_collection(collection_name=\"accommodation_images\", dim=len(emb))\n",
        "                insert_embedding(coll, id=image_id, vector=emb)\n",
        "            \n",
        "            _append_to_backfill_log(log_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "destination_ids = {\n",
        "    \"Barcelona\": \"-372490\",\n",
        "    \"Rome\": \"-126693\",\n",
        "    \"Madrid\": \"-390625\",\n",
        "    \"Paris\": \"-1456928\"\n",
        "}\n",
        "\n",
        "destination_coords = {\n",
        "    'Barcelona': {'latitude': 41.3874, 'longitude': 2.1686},\n",
        "    'Paris': {'latitude': 48.8575, 'longitude': 2.3514},\n",
        "    'Madrid': {'latitude': 40.4167, 'longitude': 3.7033},\n",
        "    'Rome': {'latitude': 41.8967, 'longitude': 12.4822}\n",
        "}\n",
        "\n",
        "accommodation_endpoint = \"https://booking-com15.p.rapidapi.com/api/v1/hotels/searchHotels\"\n",
        "weather_endpoint = 'https://archive-api.open-meteo.com/v1/archive'\n",
        "\n",
        "headers = {\n",
        "    \"x-rapidapi-key\": os.environ[\"RAPID_API_KEY\"],\n",
        "    \"x-rapidapi-host\": os.environ[\"RAPID_API_HOST\"]\n",
        "}\n",
        "\n",
        "accommodation_query = {\n",
        "    \"dest_id\": '',\n",
        "    \"search_type\": \"CITY\",\n",
        "    \"arrival_date\": '',\n",
        "    \"departure_date\": '',\n",
        "    \"adults\": \"2\",\n",
        "    \"children_age\": \"0\",\n",
        "    \"room_qty\": \"1\",\n",
        "    \"page_number\": \"1\",\n",
        "    \"units\": \"metric\",\n",
        "    \"temperature_unit\": \"c\",\n",
        "    \"languagecode\": \"en-us\",\n",
        "    \"currency_code\": \"EUR\"\n",
        "}\n",
        "\n",
        "weather_metrics = 'temperature_2m,rain,snowfall,precipitation,cloud_cover,wind_speed_10m,sunshine_duration'\n",
        "\n",
        "weather_query = {\n",
        "    'latitude': '',\n",
        "    'longitude': '',\n",
        "    'hourly': weather_metrics,\n",
        "    'start_date': '',\n",
        "    'end_date': ''\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize DataLakeServiceClient once\n",
        "datalake_client = DataLakeServiceClient.from_connection_string(connection_string)\n",
        "file_system_client = datalake_client.get_file_system_client(storage_container_name)\n",
        "engine = create_engine(POSTGRES_URL)\n",
        "connections.connect(alias=\"default\", host='localhost', port='19530')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# parameters\n",
        "start = datetime.strptime('2025-07-01', '%Y-%m-%d')\n",
        "end = datetime.strptime('2025-07-02', '%Y-%m-%d')\n",
        "cities = list(destination_ids.keys())\n",
        "\n",
        "# sync on the fly\n",
        "get_and_sync_accommodation(file_system_client, spark, start, end, destination_ids, accommodation_query, headers, 'accommodation_schema.json')\n",
        "get_and_sync_weather(file_system_client, spark, start, end, destination_coords, weather_query, 'weather_schema.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "backfill_trusted_zone(file_system_client, 'accommodation/', 'accommodation_schema.json')\n",
        "backfill_trusted_zone(file_system_client, 'accommodation_images/')\n",
        "backfill_trusted_zone(file_system_client, 'weather/', 'weather_schema.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_fetch_files(Collection('accommodation_images'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2127 [00:00<?, ?it/s]/var/folders/_m/dy6l5nmj1h986_4zdymz20240000gn/T/ipykernel_18608/1698457556.py:266: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"inserted_at\": datetime.utcnow()\n",
            "100%|██████████| 2127/2127 [09:30<00:00,  3.73it/s]\n"
          ]
        }
      ],
      "source": [
        "backfill_exploitation_zone(file_system_client, spark, 'accommodation')\n",
        "backfill_exploitation_zone(file_system_client, spark, 'accommodation_images')\n",
        "backfill_exploitation_zone(file_system_client, spark, 'weather')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tpc-di",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
