{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT5BL9nf73Yu",
        "outputId": "f0396722-8b21-4843-d219-c3611c363660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.25.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.2.3)\n",
            "Collecting azure-core>=1.30.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting cryptography>=2.1.4 (from azure-storage-blob)\n",
            "  Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (4.11.0)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (1.26.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-core>=1.30.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
            "Downloading azure_storage_blob-12.25.0-py3-none-any.whl (406 kB)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, cryptography, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.32.0 azure-storage-blob-12.25.0 cryptography-44.0.2 isodate-0.7.2\n"
          ]
        }
      ],
      "source": [
        "! pip install azure-storage-blob requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgO-3_DDtKM-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import hashlib\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import requests as req\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, explode, col, array, lit, transform\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from hdfs import InsecureClient as HdfsClient\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZQOLKA6O788v"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/05/20 14:28:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "connection_string = os.getenv('AZURE_CONNECTION_STRING')\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BDMProject\") \\\n",
        "    .master(\"spark://localhost:7077\")\\\n",
        "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"OFF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHQMnHfmq29F"
      },
      "source": [
        "# Transportation\n",
        "\n",
        "Find out more in: https://openrouteservice.org/dev/#/api-docs/optimization/post\n",
        "Alternatives: https://github.com/graphhopper/graphhopper/blob/master/README.md#Map-Matching\n",
        "https://github.com/VROOM-Project/vroom/blob/master/docs/API.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listing root directory:\n",
            "['exploitation_zone', 'tmp']\n",
            "File written to /tmp/test_hdfs_connection.txt\n",
            "File content:\n",
            "HDFS connection successful!\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def test_spark_connection() -> bool:\n",
        "    \"\"\"\n",
        "    Verifies the Spark session is active by performing a simple operation.\n",
        "    Returns True if the session is responsive, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Simple operation: create a small DataFrame and collect\n",
        "        count = spark.range(0, 1).count()\n",
        "        print (count == 1)\n",
        "    except Exception:\n",
        "        return print(False)\n",
        "\n",
        "def test_hdfs_connection():\n",
        "    try:\n",
        "        hdfs_url = os.environ['HDFS_URL']\n",
        "        client = InsecureClient(hdfs_url, user='hdfs')  # use appropriate user\n",
        "\n",
        "        # List root directory\n",
        "        print(\"Listing root directory:\")\n",
        "        print(client.list('/'))\n",
        "\n",
        "        # Write a test file\n",
        "        test_path = '/tmp/test_hdfs_connection.txt'\n",
        "        test_content = 'HDFS connection successful!'\n",
        "        client.write(test_path, data=test_content, overwrite=True)\n",
        "        print(f\"File written to {test_path}\")\n",
        "\n",
        "        # Read back the file\n",
        "        with client.read(test_path, encoding='utf-8') as reader:\n",
        "            content = reader.read()\n",
        "            print(\"File content:\")\n",
        "            print(content)\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ HDFS connection failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run test\n",
        "if __name__ == '__main__':\n",
        "    test_hdfs_connection()\n",
        "    test_spark_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_struct(flat_schema: dict) -> StructType:\n",
        "    fields = []\n",
        "    for name, typ in flat_schema.items():\n",
        "        if isinstance(typ, list):\n",
        "            elem = typ[0]\n",
        "            if isinstance(elem, dict):\n",
        "                struct = build_struct(elem)\n",
        "                fields.append(StructField(name, ArrayType(struct), True))\n",
        "            else:\n",
        "                spark_type = {'int': IntegerType(), 'float': DoubleType(), 'str': StringType(), 'bool': BooleanType()}.get(elem, StringType())\n",
        "                fields.append(StructField(name, ArrayType(spark_type), True))\n",
        "        else:\n",
        "            spark_type = {'int': IntegerType(), 'float': DoubleType(), 'str': StringType(), 'bool': BooleanType()}.get(typ, StringType())\n",
        "            fields.append(StructField(name, spark_type, True))\n",
        "    return StructType(fields)\n",
        "\n",
        "def string_to_sha256(text: str) -> str:\n",
        "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
        "\n",
        "def flatten_df(df):\n",
        "    from pyspark.sql.types import StructType\n",
        "    flat_cols = []\n",
        "    nested_cols = []\n",
        "    for field in df.schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            nested_cols.append(field.name)\n",
        "        else:\n",
        "            flat_cols.append(col(field.name))\n",
        "    for nested in nested_cols:\n",
        "        for f in df.schema[nested].dataType.fields:\n",
        "            flat_cols.append(col(f\"{nested}.{f.name}\").alias(f\"{nested}_{f.name}\"))\n",
        "    return df.select(flat_cols)\n",
        "\n",
        "# Configuración del Data Lake\n",
        "# Asume que 'file_system_name' es el filesystem de Delta Lake\n",
        "def get_data_lake_service(account_url: str, credential) -> DataLakeServiceClient:\n",
        "    return DataLakeServiceClient(account_url=account_url, credential=credential)\n",
        "\n",
        "@udf(StringType())\n",
        "def sha256_udf(url: str) -> str:\n",
        "    return hashlib.sha256(url.encode('utf-8')).hexdigest() if url else None\n",
        "\n",
        "@udf(StringType())\n",
        "def standardized_hours_udf(ts: str) -> str:\n",
        "    return datetime.fromisoformat(ts).strftime('%H:%M') if ts else None\n",
        "\n",
        "\n",
        "def file_exists(fs_client, path: str) -> bool:\n",
        "    try:\n",
        "        file_client = fs_client.get_file_client(path)\n",
        "        return file_client.exists()\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def upload_file(fs_client, path: str, data: bytes, overwrite=True) -> None:\n",
        "    file_client = fs_client.get_file_client(path)\n",
        "    if overwrite and file_client.exists():\n",
        "        file_client.delete_file()\n",
        "    file_client.create_file()\n",
        "    file_client.append_data(data, offset=0)\n",
        "    file_client.flush_data(len(data))\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# FLATTEN, CAST, SCHEMA Y COMPRESIÓN DE IMÁGENES (sin cambios)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def load_json_schema(path: str) -> dict:\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def compress_image(image_bytes: bytes, max_width: int = 1024, quality: int = 75) -> bytes:\n",
        "    with Image.open(BytesIO(image_bytes)) as img:\n",
        "        if img.mode in (\"RGBA\", \"P\"):\n",
        "            img = img.convert(\"RGB\")\n",
        "        if img.width > max_width:\n",
        "            ratio = max_width / float(img.width)\n",
        "            new_height = int(img.height * ratio)\n",
        "            img = img.resize((max_width, new_height), Image.LANCZOS)\n",
        "        buffer = BytesIO()\n",
        "        img.save(buffer, format=\"JPEG\", quality=quality, optimize=True)\n",
        "        return buffer.getvalue()\n",
        "\n",
        "def process_accommodation_record(record: dict, schema: dict) -> dict:\n",
        "    # Crear DF inferido\n",
        "    accommodation_schema_dict = json.loads(open('accommodation_schema.json').read())\n",
        "    accommodation_struct = build_struct(accommodation_schema_dict)\n",
        "     # Crear DF completo e inferido\n",
        "    df = spark.read.json(spark.sparkContext.parallelize([json.dumps(record)]))\n",
        "\n",
        "    # Extraer columnas planas y hasta 3 niveles de anidación\n",
        "\n",
        "    all_cols = []\n",
        "    for field in df.schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            for subfield in field.dataType.fields:\n",
        "                if isinstance(subfield.dataType, StructType):\n",
        "                    for subsubfield in subfield.dataType.fields:\n",
        "                        if isinstance(subsubfield.dataType, StructType):\n",
        "                            for subsubsubfield in subsubfield.dataType.fields:\n",
        "                                all_cols.append(\n",
        "                                    col(f\"{field.name}.{subfield.name}.{subsubfield.name}.{subsubsubfield.name}\").alias(f\"{field.name}_{subfield.name}_{subsubfield.name}_{subsubsubfield.name}\")\n",
        "                                )\n",
        "                        else:\n",
        "                            all_cols.append(\n",
        "                                col(f\"{field.name}.{subfield.name}.{subsubfield.name}\").alias(f\"{field.name}_{subfield.name}_{subsubfield.name}\")\n",
        "                            )\n",
        "                else:\n",
        "                    all_cols.append(\n",
        "                        col(f\"{field.name}.{subfield.name}\").alias(f\"{field.name}_{subfield.name}\")\n",
        "                    )\n",
        "        else:\n",
        "            all_cols.append(col(field.name))\n",
        "\n",
        "    df_flat = df.select(*all_cols)\n",
        "\n",
        "    # Generar hashes de fotos en Python\n",
        "    photo_urls = record.get(\"property\", {}).get(\"photoUrls\", [])\n",
        "    photo_hashes = [string_to_sha256(u) for u in photo_urls]\n",
        "    df_final = df_flat.withColumn(\"property_photoHash\", lit(photo_hashes))\n",
        "\n",
        "    # Serializar y validar con esquema oficial\n",
        "    json_flat = df_final.toJSON().first()\n",
        "    df_valid = spark.read.schema(accommodation_struct).json(spark.sparkContext.parallelize([json_flat]))\n",
        "    return df_valid.collect()[0].asDict()\n",
        "\n",
        "# --- Procesar registro de clima con Spark flatten y luego aplicar esquema ---\n",
        "def process_weather_record(raw: dict, schema: dict) -> dict:\n",
        "    weather_schema_dict = json.loads(open('weather_schema.json').read())\n",
        "    weather_struct = build_struct(weather_schema_dict)\n",
        "    raw_hourly = raw['hourly']\n",
        "    raw_hourly['timestamp'] = raw_hourly['time']\n",
        "    raw_hourly['time'] = [datetime.fromisoformat(t).strftime('%H:%M') for t in raw_hourly['time']]\n",
        "    \n",
        "    data_rows = [dict(zip(raw_hourly.keys(), values)) for values in zip(*raw_hourly.values())]\n",
        "\n",
        "    # 4. Crear el DataFrame con esquema aplicado\n",
        "    df_valid = spark.createDataFrame(data_rows, schema=weather_struct)\n",
        "    all_rows = [row.asDict() for row in df_valid.collect()]\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "# --- Procesado de imágenes sin cambios ---\n",
        "def process_accommodation_images(photo_urls: list, fs_client, city: str) -> None:\n",
        "    for url in photo_urls:\n",
        "        sha = string_to_sha256(url)\n",
        "        path = f\"landing_zone/accommodation_images/{city}/{sha}.jpg\"\n",
        "        trusted_path = f\"trusted_zone/accommodation_images/{city}/{sha}.jpg\"\n",
        "        if file_exists(fs_client, path) and file_exists(fs_client, trusted_path):\n",
        "            continue\n",
        "        if file_exists(fs_client, path):\n",
        "            data = fs_client.get_file_client(path).download_file().readall()\n",
        "        else:\n",
        "            res = req.get(url, stream=True)\n",
        "            try:\n",
        "                res.raise_for_status()\n",
        "            except:\n",
        "                continue\n",
        "            data = res.content\n",
        "            upload_file(fs_client, path, data)\n",
        "        compressed = compress_image(data, max_width=800, quality=70)\n",
        "        upload_file(fs_client, trusted_path, compressed)\n",
        "\n",
        "BACKFILL_LOG = \"/exploitation_zone/backfilled_files.txt\"\n",
        "\n",
        "def _load_backfill_log(hdfs_client: InsecureClient):\n",
        "    if hdfs_client.status(BACKFILL_LOG, strict=False):\n",
        "        with hdfs_client.read(BACKFILL_LOG) as reader:\n",
        "            return reader.read().decode('utf-8').splitlines()\n",
        "    return []\n",
        "\n",
        "def _append_to_backfill_log(hdfs_client: InsecureClient, log_entry: str):\n",
        "    entries = _load_backfill_log(hdfs_client)\n",
        "    if log_entry not in entries:\n",
        "        entries.append(log_entry)\n",
        "        hdfs_client.write(BACKFILL_LOG, \"\\n\".join(entries).encode('utf-8'), overwrite=True)\n",
        "\n",
        "\n",
        "def _load_parquet(spark, hdfs_client, data_type, city, filename):\n",
        "    hdfs_path = f\"/exploitation_zone/{data_type}/{city}/{filename}\"\n",
        "    \n",
        "    if not hdfs_client.status(hdfs_path, strict=False):\n",
        "        return None  # El archivo aún no existe\n",
        "\n",
        "    # Copiar archivo HDFS → local temporal\n",
        "    local_temp = f\"/tmp/{filename}_read\"\n",
        "    hdfs_client.download(hdfs_path, local_temp, overwrite=True)\n",
        "\n",
        "    # Leer el Parquet localmente\n",
        "    return spark.read.parquet(local_temp)\n",
        "\n",
        "\n",
        "def _write_parquet_to_hdfs(df, hdfs_client: InsecureClient, data_type: str, city: str, filename: str):\n",
        "    local_temp = f\"/tmp/{filename}\"\n",
        "    df.write.mode('append').parquet(local_temp)\n",
        "    hdfs_target = f\"/exploitation_zone/{data_type}/{city}/{filename}\"\n",
        "    hdfs_client.delete(hdfs_target, recursive=True)\n",
        "    hdfs_client.upload(hdfs_target, local_temp, overwrite=True)\n",
        "    log_entry = f\"{data_type}/{city}/{filename}\"\n",
        "    _append_to_backfill_log(hdfs_client, log_entry)\n",
        "\n",
        "def _read_json_df(dl_client, spark, path):\n",
        "    \"\"\"\n",
        "    Download JSON from `path`, parse it, and return a Spark DataFrame.\n",
        "    \"\"\"\n",
        "    data_bytes = dl_client.get_file_client(path).download_file().readall()\n",
        "    docs = json.loads(data_bytes.decode('utf-8'))\n",
        "    # Wrap single dict or list of dicts into a list\n",
        "    records = docs if isinstance(docs, list) else [docs]\n",
        "    json_rdd = spark.sparkContext.parallelize([json.dumps(record) for record in records])\n",
        "    return spark.read.json(json_rdd)\n",
        "\n",
        "\n",
        "def _process_parquet_backfill(\n",
        "    dl_client, hdfs_client, spark,\n",
        "    data_type, file_paths,\n",
        "    data_file, log_entries\n",
        "):\n",
        "    \"\"\"\n",
        "    Load existing parquet per city (named by `data_file`), union with new JSON files, write once per city.\n",
        "    \"\"\"\n",
        "    # Group file paths by city\n",
        "    files_by_city = defaultdict(list)\n",
        "    for full_path, city in file_paths:\n",
        "        log_key = f\"{data_type}/{city}/{full_path.split('/')[-1]}\"\n",
        "        if log_key not in log_entries:\n",
        "            files_by_city[city].append(full_path)\n",
        "\n",
        "    for city, paths in files_by_city.items():\n",
        "        # Load existing data once\n",
        "        existing = _load_parquet(spark, hdfs_client, data_type, city, data_file)\n",
        "        new_dfs = []\n",
        "        for path in tqdm(paths, desc=f\"Loading {data_type} files in {city}\"):\n",
        "            df = _read_json_df(dl_client, spark, path)\n",
        "            new_dfs.append(df)\n",
        "        if not new_dfs:\n",
        "            continue\n",
        "        # Combine all new and existing\n",
        "        combined = new_dfs[0] if existing is None else existing.unionByName(new_dfs[0], allowMissingColumns=True)\n",
        "        for df in tqdm(new_dfs[1:], desc=f\"Merging {data_type} files in {city}\"):\n",
        "            combined = combined.unionByName(df, allowMissingColumns=True)\n",
        "        # Write back to HDFS\n",
        "        _write_parquet_to_hdfs(combined, hdfs_client, data_type, city, data_file)\n",
        "        # Append each file to log\n",
        "        for path in paths:\n",
        "            log_key = f\"{data_type}/{city}/{path.split('/')[-1]}\"\n",
        "            _append_to_backfill_log(hdfs_client, log_key)\n",
        "\n",
        "def get_and_sync_accommodation(\n",
        "    fs,\n",
        "    hdfs_client: InsecureClient,\n",
        "    spark: SparkSession,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    cities: dict,\n",
        "    query_template: dict,\n",
        "    headers:dict,\n",
        "    schema_file: str\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    delta = timedelta(days=1)\n",
        "\n",
        "    for single_date in tqdm([start + i * delta for i in range((end - start).days + 1)]):\n",
        "        arrival = single_date.strftime('%Y-%m-%d')\n",
        "        departure = (single_date + delta).strftime('%Y-%m-%d')\n",
        "        for city, dest_id in tqdm(cities.items()):\n",
        "            landing_path = f\"landing_zone/accommodation/{city}/{arrival}_{departure}.json\"\n",
        "            trusted_path = f\"trusted_zone/accommodation/{city}/{arrival}_{departure}.json\"\n",
        "\n",
        "            # filtro: si existe en landing y trusted, nada que hacer\n",
        "            if file_exists(fs, landing_path) and file_exists(fs, trusted_path):\n",
        "                continue\n",
        "\n",
        "            # obtener JSON: de landing o API\n",
        "            if file_exists(fs, landing_path):\n",
        "                raw = fs.get_file_client(landing_path).download_file().readall().decode('utf-8')\n",
        "                data = json.loads(raw)\n",
        "            else:\n",
        "                params = dict(query_template, dest_id=dest_id, arrival_date=arrival, departure_date=departure)\n",
        "                res = req.get(accommodation_endpoint, params=params, headers=headers)\n",
        "                time.sleep(10)\n",
        "                res.raise_for_status()\n",
        "                data = res.json()\n",
        "                upload_file(fs, landing_path, json.dumps(data).encode('utf-8'))\n",
        "            # procesar imágenes\n",
        "            photo_urls = [u for h in data['data']['hotels'] for u in h['property']['photoUrls']]\n",
        "            process_accommodation_images(photo_urls, fs, city)\n",
        "            # transformar y guardar registros en zona confiable\n",
        "            docs = [process_accommodation_record(r, schema) for r in data['data']['hotels']]\n",
        "            upload_file(fs, trusted_path, json.dumps(docs).encode('utf-8'))\n",
        "\n",
        "            exploitation_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(d) for d in docs]))\n",
        "\n",
        "            existing_df = _load_parquet(spark, hdfs_client, 'accommodation', city, 'AccommodationData.parquet')\n",
        "            if existing_df is None:\n",
        "                combined = exploitation_df\n",
        "            else:\n",
        "                combined = existing_df.unionByName(exploitation_df, allowMissingColumns=True).dropDuplicates()\n",
        "            # write back\n",
        "            _write_parquet_to_hdfs(combined, hdfs_client, 'accommodation', city, 'AccommodationData.parquet')\n",
        "\n",
        "def get_and_sync_weather(\n",
        "    fs,\n",
        "    hdfs_client: InsecureClient,\n",
        "    spark: SparkSession,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    coords: dict,\n",
        "    query_template: dict,\n",
        "    schema_file: str\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    delta = timedelta(days=1)\n",
        "\n",
        "    for single_date in tqdm([start + i * delta for i in range((end - start).days + 1)]):\n",
        "        prev_start = (single_date - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "        for city, coord in coords.items():\n",
        "            landing_path = f\"landing_zone/weather/{city}/{prev_start}.json\"\n",
        "            trusted_path = f\"trusted_zone/landing_zone/weather/{city}/{prev_start}.json\"\n",
        "\n",
        "            # filtro: si existe en landing y trusted, nada que hacer\n",
        "            if file_exists(fs, landing_path) and file_exists(fs, trusted_path):\n",
        "                continue\n",
        "\n",
        "            # obtener JSON: de landing o API\n",
        "            if file_exists(fs, landing_path):\n",
        "                raw = fs.get_file_client(landing_path).download_file().readall().decode('utf-8')\n",
        "                data = json.loads(raw)\n",
        "            else:\n",
        "                params = dict(query_template, latitude=coord['latitude'], longitude=coord['longitude'], start_date=prev_start, end_date=prev_start)\n",
        "                res = req.get(weather_endpoint, params=params)\n",
        "                time.sleep(10)\n",
        "                res.raise_for_status()\n",
        "                data = res.json()\n",
        "                upload_file(fs, landing_path, json.dumps(data).encode('utf-8'))\n",
        "\n",
        "            # transformar y guardar en zona confiable\n",
        "            doc = process_weather_record(data, schema)\n",
        "            upload_file(fs, trusted_path, json.dumps(doc).encode('utf-8'))\n",
        "\n",
        "            exploitation_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(doc)]))\n",
        "\n",
        "            existing_df = _load_parquet(spark, hdfs_client, 'weather', city, 'WeatherData.parquet')\n",
        "            if existing_df is None:\n",
        "                combined = exploitation_df\n",
        "            else:\n",
        "                combined = existing_df.unionByName(exploitation_df, allowMissingColumns=True).dropDuplicates()\n",
        "            _write_parquet_to_hdfs(combined, hdfs_client, 'weather', city, 'WeatherData.parquet')\n",
        "\n",
        "def list_files(fs_client, prefix: str) -> list:\n",
        "    return [p.name for p in fs_client.get_paths(path=prefix) if not p.is_directory]\n",
        "\n",
        "\n",
        "def find_missing_blobs(\n",
        "    fs,\n",
        "    landing_prefix: str\n",
        ") -> list:\n",
        "    missing = []\n",
        "    for path in list_files(fs, 'landing_zone/' + landing_prefix):\n",
        "        trusted_path = f\"trusted_zone/{path.replace('landing_zone/', '', 1)}\"\n",
        "        if not file_exists(fs, trusted_path):\n",
        "            missing.append(path)\n",
        "    return missing\n",
        "\n",
        "\n",
        "def backfill_trusted_zone(\n",
        "    fs,\n",
        "    landing_prefix: str,\n",
        "    schema_file: str = None\n",
        ") -> None:\n",
        "    schema = load_json_schema(schema_file) if schema_file else None\n",
        "    to_fill = find_missing_blobs(fs, landing_prefix)\n",
        "\n",
        "    for landing_path in tqdm(to_fill):\n",
        "        trusted_path = f\"trusted_zone/{landing_path.replace('landing_zone/', '', 1)}\"\n",
        "        data_bytes = fs.get_file_client(landing_path).download_file().readall()\n",
        "\n",
        "        # imágenes de alojamiento\n",
        "        if landing_path.startswith('landing_zone/accommodation_images/'):\n",
        "            compressed = compress_image(data_bytes, max_width=800, quality=70)\n",
        "            upload_file(fs, trusted_path, compressed)\n",
        "            continue\n",
        "\n",
        "        # JSON de alojamiento\n",
        "        if landing_path.startswith('landing_zone/accommodation/'):\n",
        "            data = json.loads(data_bytes.decode('utf-8'))\n",
        "            docs = [process_accommodation_record(r, schema) for r in data['data']['hotels']]\n",
        "            upload_file(fs, trusted_path, json.dumps(docs).encode('utf-8'))\n",
        "            continue\n",
        "\n",
        "        # JSON de clima\n",
        "        if landing_path.startswith('landing_zone/weather/'):\n",
        "            data = json.loads(data_bytes.decode('utf-8'))\n",
        "            doc = process_weather_record(data, schema)\n",
        "            upload_file(fs, trusted_path, json.dumps(doc).encode('utf-8'))\n",
        "\n",
        "def backfill_exploitation_zone(\n",
        "    dl_client,\n",
        "    hdfs_client: InsecureClient,\n",
        "    spark: SparkSession,\n",
        "    data_type: str,\n",
        "    data_file: str = None\n",
        "):\n",
        "    # Load log entries once\n",
        "    log_entries = _load_backfill_log(hdfs_client)\n",
        "    # List all files under trusted_zone\n",
        "    all_paths = list_files(dl_client, \"trusted_zone/\")\n",
        "    # Filter relevant paths\n",
        "    qualified = []  # tuples of (full_path, city)\n",
        "    for path in all_paths:\n",
        "        parts = path.split('/')\n",
        "        d_type, city = parts[-3], parts[-2]\n",
        "        if d_type != data_type:\n",
        "            continue\n",
        "        full_path = f\"trusted_zone/{data_type}/{city}/{parts[-1]}\"\n",
        "        qualified.append((full_path, city))\n",
        "\n",
        "    if data_type in (\"accommodation\", \"weather\"):\n",
        "        _process_parquet_backfill(\n",
        "            dl_client, hdfs_client, spark,\n",
        "            data_type, qualified,\n",
        "            data_file, log_entries\n",
        "        )\n",
        "\n",
        "    elif data_type == \"accommodation_images\":\n",
        "        # Direct copy of images\n",
        "        for full_path, city in tqdm(qualified, desc=\"Copying images\"):\n",
        "            file_name = full_path.split('/')[-1]\n",
        "            log_key = f\"{data_type}/{city}/{file_name}\"\n",
        "            if log_key in log_entries:\n",
        "                continue\n",
        "            data_bytes = dl_client.get_file_client(full_path).download_file().readall()\n",
        "            hdfs_target = f\"/exploitation_zone/accommodation_images/{city}/{file_name}\"\n",
        "            hdfs_client.write(hdfs_target, data_bytes, overwrite=True)\n",
        "            _append_to_backfill_log(hdfs_client, log_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "destination_ids = {\n",
        "    \"Barcelona\": \"-372490\",\n",
        "    \"Rome\": \"-126693\",\n",
        "    \"Madrid\": \"-390625\",\n",
        "    \"Paris\": \"-1456928\"\n",
        "}\n",
        "\n",
        "destination_coords = {\n",
        "    'Barcelona': {'latitude': 41.3874, 'longitude': 2.1686},\n",
        "    'Paris': {'latitude': 48.8575, 'longitude': 2.3514},\n",
        "    'Madrid': {'latitude': 40.4167, 'longitude': 3.7033},\n",
        "    'Rome': {'latitude': 41.8967, 'longitude': 12.4822}\n",
        "}\n",
        "\n",
        "accommodation_endpoint = \"https://booking-com15.p.rapidapi.com/api/v1/hotels/searchHotels\"\n",
        "weather_endpoint = 'https://archive-api.open-meteo.com/v1/archive'\n",
        "\n",
        "headers = {\n",
        "    \"x-rapidapi-key\": os.environ[\"RAPID_API_KEY\"],\n",
        "    \"x-rapidapi-host\": os.environ[\"RAPID_API_HOST\"]\n",
        "}\n",
        "\n",
        "accommodation_query = {\n",
        "    \"dest_id\": '',\n",
        "    \"search_type\": \"CITY\",\n",
        "    \"arrival_date\": '',\n",
        "    \"departure_date\": '',\n",
        "    \"adults\": \"2\",\n",
        "    \"children_age\": \"0\",\n",
        "    \"room_qty\": \"1\",\n",
        "    \"page_number\": \"1\",\n",
        "    \"units\": \"metric\",\n",
        "    \"temperature_unit\": \"c\",\n",
        "    \"languagecode\": \"en-us\",\n",
        "    \"currency_code\": \"EUR\"\n",
        "}\n",
        "\n",
        "weather_metrics = 'temperature_2m,rain,snowfall,precipitation,cloud_cover,wind_speed_10m,sunshine_duration'\n",
        "\n",
        "weather_query = {\n",
        "    'latitude': '',\n",
        "    'longitude': '',\n",
        "    'hourly': weather_metrics,\n",
        "    'start_date': '',\n",
        "    'end_date': ''\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize DataLakeServiceClient once\n",
        "datalake_client = DataLakeServiceClient.from_connection_string(connection_string)\n",
        "file_system_client = datalake_client.get_file_system_client(storage_container_name)\n",
        "hdfs_service = HdfsClient(os.environ['HDFS_URL'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 13.69it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 18.87it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n",
            "100%|██████████| 2/2 [01:43<00:00, 51.90s/it]                                   \n"
          ]
        }
      ],
      "source": [
        "# parameters\n",
        "start = datetime.strptime('2025-06-29', '%Y-%m-%d')\n",
        "end = datetime.strptime('2025-06-30', '%Y-%m-%d')\n",
        "cities = list(destination_ids.keys())\n",
        "\n",
        "# sync on the fly\n",
        "get_and_sync_accommodation(file_system_client, hdfs_service, spark, start, end, destination_ids, accommodation_query, headers, 'accommodation_schema.json')\n",
        "get_and_sync_weather(file_system_client, hdfs_service, spark, start, end, destination_coords, weather_query, 'weather_schema.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "backfill_trusted_zone(file_system_client, 'accommodation/', 'accommodation_schema.json')\n",
        "backfill_trusted_zone(file_system_client, 'accommodation_images/')\n",
        "backfill_trusted_zone(file_system_client, 'weather/', 'weather_schema.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying images: 100%|██████████| 2048/2048 [00:00<00:00, 32475.39it/s]\n",
            "Loading weather files in Barcelona: 100%|██████████| 101/101 [00:37<00:00,  2.71it/s]\n",
            "Merging weather files in Barcelona: 100%|██████████| 100/100 [00:01<00:00, 55.39it/s]\n",
            "Loading weather files in Madrid: 100%|██████████| 101/101 [00:29<00:00,  3.39it/s]\n",
            "Merging weather files in Madrid: 100%|██████████| 100/100 [00:01<00:00, 66.00it/s]\n",
            "Loading weather files in Paris: 100%|██████████| 101/101 [00:30<00:00,  3.32it/s]\n",
            "Merging weather files in Paris: 100%|██████████| 100/100 [00:01<00:00, 65.75it/s]\n",
            "Loading weather files in Rome: 100%|██████████| 101/101 [00:23<00:00,  4.34it/s]\n",
            "Merging weather files in Rome: 100%|██████████| 100/100 [00:00<00:00, 107.83it/s]\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "backfill_exploitation_zone(file_system_client, hdfs_service, spark, 'accommodation', 'AccommodationData.parquet')\n",
        "backfill_exploitation_zone(file_system_client, hdfs_service, spark, 'accommodation_images')\n",
        "backfill_exploitation_zone(file_system_client, hdfs_service, spark, 'weather', 'WeatherData.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "99.0"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_type = 'weather'\n",
        "city = 'Barcelona'\n",
        "filename = 'WeatherData.parquet'\n",
        "path = f\"hdfs://localhost:9000/exploitation_zone/{data_type}/{city}/{filename}\"\n",
        "limit = 20\n",
        "df = spark.read.parquet(path)\n",
        "df.select('timestamp').distinct().count()/24"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tpc-di",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
