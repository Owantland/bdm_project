{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT5BL9nf73Yu",
        "outputId": "f0396722-8b21-4843-d219-c3611c363660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.25.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.2.3)\n",
            "Collecting azure-core>=1.30.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting cryptography>=2.1.4 (from azure-storage-blob)\n",
            "  Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (4.11.0)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (1.26.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.11.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-core>=1.30.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
            "Downloading azure_storage_blob-12.25.0-py3-none-any.whl (406 kB)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, cryptography, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.32.0 azure-storage-blob-12.25.0 cryptography-44.0.2 isodate-0.7.2\n"
          ]
        }
      ],
      "source": [
        "! pip install azure-storage-blob requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgO-3_DDtKM-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "from hdfs import InsecureClient as HdfsClient\n",
        "import requests as req\n",
        "from azure.storage.blob import BlobServiceClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZQOLKA6O788v"
      },
      "outputs": [],
      "source": [
        "connection_string = os.getenv('AZURE_CONNECTION_STRING')\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHQMnHfmq29F"
      },
      "source": [
        "# Transportation\n",
        "\n",
        "Find out more in: https://openrouteservice.org/dev/#/api-docs/optimization/post\n",
        "Alternatives: https://github.com/graphhopper/graphhopper/blob/master/README.md#Map-Matching\n",
        "https://github.com/VROOM-Project/vroom/blob/master/docs/API.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listing root directory:\n",
            "[]\n",
            "File written to /tmp/test_hdfs_connection.txt\n",
            "File content:\n",
            "HDFS connection successful!\n"
          ]
        }
      ],
      "source": [
        "from hdfs import InsecureClient\n",
        "import os\n",
        "\n",
        "def test_hdfs_connection():\n",
        "    try:\n",
        "        hdfs_url = os.environ['HDFS_URL']\n",
        "        client = InsecureClient(hdfs_url, user='hdfs')  # use appropriate user\n",
        "\n",
        "        # List root directory\n",
        "        print(\"Listing root directory:\")\n",
        "        print(client.list('/'))\n",
        "\n",
        "        # Write a test file\n",
        "        test_path = '/tmp/test_hdfs_connection.txt'\n",
        "        test_content = 'HDFS connection successful!'\n",
        "        client.write(test_path, data=test_content, overwrite=True)\n",
        "        print(f\"File written to {test_path}\")\n",
        "\n",
        "        # Read back the file\n",
        "        with client.read(test_path, encoding='utf-8') as reader:\n",
        "            content = reader.read()\n",
        "            print(\"File content:\")\n",
        "            print(content)\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ HDFS connection failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run test\n",
        "if __name__ == '__main__':\n",
        "    test_hdfs_connection()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import requests as req\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from hdfs import InsecureClient as HdfsClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "destination_ids = {\n",
        "    \"Barcelona\": \"-372490\",\n",
        "    \"Rome\": \"-126693\",\n",
        "    \"Madrid\": \"-390625\",\n",
        "    \"Paris\": \"-1456928\"\n",
        "}\n",
        "\n",
        "destination_coords = {\n",
        "    'Barcelona': {'latitude': 41.3874, 'longitude': 2.1686},\n",
        "    'Paris': {'latitude': 48.8575, 'longitude': 2.3514},\n",
        "    'Madrid': {'latitude': 40.4167, 'longitude': 3.7033},\n",
        "    'Rome': {'latitude': 41.8967, 'longitude': 12.4822}\n",
        "}\n",
        "\n",
        "accommodation_endpoint = \"https://booking-com15.p.rapidapi.com/api/v1/hotels/searchHotels\"\n",
        "weather_endpoint = 'https://archive-api.open-meteo.com/v1/archive'\n",
        "\n",
        "headers = {\n",
        "    \"x-rapidapi-key\": os.environ[\"RAPID_API_KEY\"],\n",
        "    \"x-rapidapi-host\": os.environ[\"RAPID_API_HOST\"]\n",
        "}\n",
        "\n",
        "accommodation_query = {\n",
        "    \"dest_id\": '',\n",
        "    \"search_type\": \"CITY\",\n",
        "    \"arrival_date\": '',\n",
        "    \"departure_date\": '',\n",
        "    \"adults\": \"2\",\n",
        "    \"children_age\": \"0\",\n",
        "    \"room_qty\": \"1\",\n",
        "    \"page_number\": \"1\",\n",
        "    \"units\": \"metric\",\n",
        "    \"temperature_unit\": \"c\",\n",
        "    \"languagecode\": \"en-us\",\n",
        "    \"currency_code\": \"EUR\"\n",
        "}\n",
        "\n",
        "weather_metrics = 'temperature_2m,rain,snowfall,precipitation,cloud_cover,wind_speed_10m,sunshine_duration'\n",
        "\n",
        "weather_query = {\n",
        "    'latitude': '',\n",
        "    'longitude': '',\n",
        "    'hourly': weather_metrics,\n",
        "    'start_date': '',\n",
        "    'end_date': ''\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "def string_to_sha256(text: str) -> str:\n",
        "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
        "\n",
        "def upload_blob(container_client, blob_name: str, data: bytes) -> None:\n",
        "    \"\"\"\n",
        "    Sube datos binarios a Azure Blob bajo blob_name.\n",
        "    \"\"\"\n",
        "    container_client.upload_blob(name=blob_name, data=data, overwrite=True)\n",
        "\n",
        "# HDFS helper\n",
        "def save_into_hdfs(hdfs_client: HdfsClient, data: dict, hdfs_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Serializa dict a JSON y escribe directamente en HDFS en la ruta dada.\n",
        "    \"\"\"\n",
        "    parent = os.path.dirname(hdfs_path)\n",
        "    if parent and not hdfs_client.status(parent, strict=False):\n",
        "        hdfs_client.makedirs(parent)\n",
        "    with hdfs_client.write(hdfs_path, encoding='utf-8', overwrite=True) as writer:\n",
        "        writer.write(json.dumps(data))\n",
        "\n",
        "# Trusted-processing utilities\n",
        "def flatten_dict(d: dict, parent_key: str = '', sep: str = '_') -> dict:\n",
        "    items = {}\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.update(flatten_dict(v, new_key, sep=sep))\n",
        "        else:\n",
        "            items[new_key] = v\n",
        "    return items\n",
        "\n",
        "def cast_value(value, expected: str):\n",
        "    try:\n",
        "        if expected == 'int': return int(value)\n",
        "        if expected == 'float': return float(value)\n",
        "        if expected == 'bool': return value in ['true','1','True'] if isinstance(value, str) else bool(value)\n",
        "        return str(value)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def enforce_schema(data: dict, schema: dict) -> dict:\n",
        "    def enforce(d, s):\n",
        "        out = {}\n",
        "        for k, exp in s.items():\n",
        "            val = d.get(k)\n",
        "            if val is None:\n",
        "                out[k] = None\n",
        "            else:\n",
        "                if isinstance(exp, dict):\n",
        "                    out[k] = enforce(val if isinstance(val, dict) else {}, exp)\n",
        "                elif isinstance(exp, list):\n",
        "                    if exp and isinstance(exp[0], dict):\n",
        "                        out[k] = [enforce(item, exp[0]) for item in val if isinstance(item, dict)]\n",
        "                    else:\n",
        "                        out[k] = [cast_value(item, exp[0]) for item in val] if isinstance(val, list) else []\n",
        "                else:\n",
        "                    out[k] = cast_value(val, exp)\n",
        "        return out\n",
        "    return enforce(data, schema)\n",
        "\n",
        "def standardized_hours(timestamp: str) -> str:\n",
        "    dt = datetime.fromisoformat(timestamp)\n",
        "    return dt.strftime('%H:%M')\n",
        "\n",
        "# Schema loaders\n",
        "def load_json_schema(path: str) -> dict:\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def process_accommodation_images(photo_urls: list, blob_container_client, city: str) -> None:\n",
        "    \"\"\"\n",
        "    Descarga y sube imágenes de alojamiento a Azure Blob.\n",
        "    \"\"\"\n",
        "    for url in photo_urls:\n",
        "        sha = string_to_sha256(url)\n",
        "        blob_name = f\"accommodation_images/{city}/{sha}.jpg\"\n",
        "        # request image\n",
        "        res = req.get(url, stream=True)\n",
        "        if res.status_code == 200:\n",
        "            upload_blob(blob_container_client, blob_name, res.content)\n",
        "\n",
        "\n",
        "# Core: process one accommodation JSON into HDFS\n",
        "\n",
        "def process_accommodation_record(record: dict, schema: dict) -> dict:\n",
        "    flat = flatten_dict(record)\n",
        "    flat['property_photoHash'] = [string_to_sha256(u) for u in flat.get('property_photoUrls', [])]\n",
        "    flat.pop('property_photoUrls', None)\n",
        "    return enforce_schema(flat, schema)\n",
        "\n",
        "# Core: process one weather JSON into HDFS\n",
        "\n",
        "def process_weather_record(raw: dict, schema: dict) -> dict:\n",
        "    hourly = raw.get('hourly', {})\n",
        "    hourly['time'] = [standardized_hours(t) for t in hourly.get('time', [])]\n",
        "    return enforce_schema(hourly, schema)\n",
        "\n",
        "\n",
        "# Missing detection: list landing blobs and corresponding HDFS paths\n",
        "def find_missing_blobs(blob_client, landing_prefix: str, hdfs_client: HdfsClient, hdfs_prefix: str):\n",
        "    missing = []\n",
        "    cities = ['Barcelona', 'Paris', 'Rome', 'Madrid']\n",
        "    for blob in blob_client.list_blobs(name_starts_with=landing_prefix):\n",
        "        for city in cities:\n",
        "            if blob.name.endswith('.json'):\n",
        "                fname = os.path.basename(blob.name)\n",
        "                hdfs_path = f\"/{hdfs_prefix}{city}/{fname}\" \n",
        "                if not hdfs_client.status(hdfs_path, strict=False):\n",
        "                    print(f\"File not found: {hdfs_path}\")\n",
        "                    missing.append(blob)\n",
        "    return missing\n",
        "\n",
        "# Landing + immediate sync\n",
        "\n",
        "def get_and_sync_accommodation(\n",
        "    blob_service_client: BlobServiceClient,\n",
        "    hdfs_client: HdfsClient,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    cities: dict,\n",
        "    query_template: dict,\n",
        "    headers: dict,\n",
        "    schema_file: str,\n",
        "    landing_container: str = 'bdmcontainerp1',\n",
        "    images_container: str = 'bdmcontainerp1',\n",
        "    hdfs_base: str = '/trusted_accommodation'\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    landing_client = blob_service_client.get_container_client(landing_container)\n",
        "    images_client = blob_service_client.get_container_client(images_container)\n",
        "    delta = timedelta(days=1)\n",
        "\n",
        "    for single_date in tqdm([start + i * delta for i in range((end - start).days + 1)]):\n",
        "        arrival = single_date.strftime('%Y-%m-%d')\n",
        "        departure = (single_date + delta).strftime('%Y-%m-%d')\n",
        "        for city, dest_id in cities.items():\n",
        "            params = dict(query_template, dest_id=dest_id, arrival_date=arrival, departure_date=departure)\n",
        "            res = req.get(accommodation_endpoint, headers=headers, params=params)\n",
        "            res.raise_for_status()\n",
        "            data = res.json()\n",
        "\n",
        "            # upload landing JSON\n",
        "            landing_blob = f\"accommodation/{city}/{arrival}_{departure}.json\"\n",
        "            upload_blob(landing_client, landing_blob, json.dumps(data).encode('utf-8'))\n",
        "\n",
        "            # upload images\n",
        "            photo_urls = [u for h in data['data']['hotels'] for u in h['property']['photoUrls']]\n",
        "            process_accommodation_images(photo_urls, images_client, city)\n",
        "\n",
        "            # sync to trusted HDFS\n",
        "            docs = [process_accommodation_record(r, schema) for r in data['data']['hotels']]\n",
        "            hdfs_path = f\"{hdfs_base}/{city}/{arrival}_{departure}.json\"\n",
        "            if not hdfs_client.status(hdfs_path, strict=False):\n",
        "                save_into_hdfs(hdfs_client, docs, hdfs_path)\n",
        "\n",
        "\n",
        "# Similarly for weather\n",
        "\n",
        "def get_and_sync_weather(\n",
        "    blob_service_client: BlobServiceClient,\n",
        "    hdfs_client: HdfsClient,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    coords: dict,\n",
        "    query_template: dict,\n",
        "    schema_file: str,\n",
        "    landing_container: str = 'bdmcontainerp1',\n",
        "    hdfs_base: str = '/trusted_weather'\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    container = blob_service_client.get_container_client(landing_container)\n",
        "    delta = timedelta(days=1)\n",
        "\n",
        "    for single_date in tqdm([start + i * delta for i in range((end - start).days + 1)]):\n",
        "        start_prev = (single_date - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "        end_prev = (single_date - timedelta(days=365) + delta).strftime('%Y-%m-%d')\n",
        "        for city, coord in coords.items():\n",
        "            # fetch landing data\n",
        "            params = dict(query_template, latitude=coord['latitude'], longitude=coord['longitude'], start_date=start_prev, end_date=end_prev)\n",
        "            res = req.get(weather_endpoint, params=params)\n",
        "            res.raise_for_status()\n",
        "            data = res.json()\n",
        "            # upload landing\n",
        "            landing_blob = f\"weather/{city}/{start_prev}.json\"\n",
        "            container.upload_blob(name=landing_blob, data=json.dumps(data), overwrite=True)\n",
        "            # sync new to trusted\n",
        "            doc = process_weather_record(data, schema)\n",
        "            hdfs_path = f\"{hdfs_base}/{city}/{start_prev}.json\"\n",
        "            if not hdfs_client.status(hdfs_path, strict=False):\n",
        "                save_into_hdfs(hdfs_client, doc, hdfs_path)\n",
        "\n",
        "# Function to backfill missing files\n",
        "def backfill_missing(\n",
        "    blob_service_client: BlobServiceClient,\n",
        "    hdfs_client: HdfsClient,\n",
        "    landing_prefix: str,\n",
        "    hdfs_prefix: str,\n",
        "    processor: callable,\n",
        "    schema_file: str\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    container = blob_service_client.get_container_client('bdmcontainerp1')\n",
        "    missing = find_missing_blobs(container, landing_prefix, hdfs_client, hdfs_prefix)\n",
        "    \n",
        "    if missing:\n",
        "        for blob in missing:\n",
        "            data = json.loads(container.get_blob_client(blob).download_blob().readall())\n",
        "            if landing_prefix.startswith('accommodation'):\n",
        "                docs = [process_accommodation_record(r, schema) for r in data['data']['hotels']]\n",
        "                save_into_hdfs(hdfs_client, docs, f\"{hdfs_prefix}/{os.path.basename(blob.name)}\")\n",
        "            else:\n",
        "                doc = process_weather_record(data, schema)\n",
        "                save_into_hdfs(hdfs_client, doc, f\"{hdfs_prefix}/{os.path.basename(blob.name)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [01:32<01:32, 92.96s/it]"
          ]
        }
      ],
      "source": [
        "blob_service = BlobServiceClient.from_connection_string(os.getenv('AZURE_CONNECTION_STRING'))\n",
        "hdfs_service = HdfsClient(os.environ['HDFS_URL'])\n",
        "\n",
        "# parameters\n",
        "start = datetime.strptime('2025-06-04', '%Y-%m-%d')\n",
        "end = datetime.strptime('2025-06-05', '%Y-%m-%d')\n",
        "cities = list(destination_ids.keys())\n",
        "\n",
        "# sync on the fly\n",
        "get_and_sync_accommodation(blob_service, hdfs_service, start, end, destination_ids, accommodation_query, headers, 'accomodation_schema.json')\n",
        "get_and_sync_weather(blob_service, hdfs_service, start, end, destination_coords, weather_query, 'weather_schema.json')\n",
        "\n",
        "# backfill any missing\n",
        "backfill_missing(blob_service, hdfs_service, 'accommodation/', 'trusted_accommodation/', process_accommodation_record, 'accomodation_schema.json')\n",
        "backfill_missing(blob_service, hdfs_service, 'weather/', 'trusted_weather/', process_weather_record, 'weather_schema.json')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tpc-di",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
