{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT5BL9nf73Yu",
        "outputId": "f0396722-8b21-4843-d219-c3611c363660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-storage-blob in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (12.25.1)\n",
            "Requirement already satisfied: requests in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: pymilvus in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.5.10)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (1.34.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (44.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (4.11.0)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-storage-blob) (0.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (1.26.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: setuptools>69 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (75.1.0)\n",
            "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (1.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (6.31.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (1.0.1)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: milvus-lite>=2.4.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from pymilvus) (2.4.12)\n",
            "Requirement already satisfied: six>=1.11.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from azure-core>=1.30.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: tqdm in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
            "Requirement already satisfied: pycparser in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/_m/dy6l5nmj1h986_4zdymz20240000gn/T/pip-req-build-4am57m26\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/_m/dy6l5nmj1h986_4zdymz20240000gn/T/pip-req-build-4am57m26\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: ftfy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (2.2.2)\n",
            "Requirement already satisfied: torchvision in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from clip==1.0) (0.17.2)\n",
            "Requirement already satisfied: wcwidth in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch->clip==1.0) (2025.5.1)\n",
            "Requirement already satisfied: numpy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torchvision->clip==1.0) (1.26.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: ftfy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from ftfy) (0.2.5)\n",
            "Requirement already satisfied: torch in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (0.17.2)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: numpy in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torchvision) (1.26.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install azure-storage-blob requests pandas pymilvus\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install torch torchvision  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wgO-3_DDtKM-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import hashlib\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import requests as req\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, explode, col, array, lit, transform\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from hdfs import InsecureClient as HdfsClient\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "import findspark\n",
        "from sqlalchemy import create_engine, inspect, text\n",
        "import io\n",
        "from PIL import Image\n",
        "import torch\n",
        "import clip\n",
        "from pymilvus import (\n",
        "    connections,\n",
        "    FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"AZURE_CONNECTION_STRING\"] = \"DefaultEndpointsProtocol=https;AccountName=bdmprojectaccount;AccountKey=B3D88p/oqHsgj58j5cI+I8bLpGnSFeUMgfVGFtCxeC8wHQKv1DDnQWGzy1/a5fMGnxmE3eYFqyji+AStJ7v2Bg==;EndpointSuffix=core.windows.net\"\n",
        "os.environ[\"RAPID_API_KEY\"] = \"356373667dmsh76bcf16aecc7050p1c4df5jsnffa784d3b2a0\"\n",
        "os.environ[\"RAPID_API_HOST\"] = \"booking-com15.p.rapidapi.com\"\n",
        "os.environ[\"OPENROUTE_KEY\"] = '5b3ce3597851110001cf624802e5afc167054c88ad92e9a69206707c'\n",
        "os.environ[\"AZURE_SAS_TOKEN\"] = 'sp=r&st=2025-05-14T15:15:23Z&se=2025-07-14T23:15:23Z&spr=https&sv=2024-11-04&sr=c&sig=Jekfw3ghnENVpQeK5e0yG8UoFivdrUNW%2BfGG8p%2FxfPQ%3D'\n",
        "storage_account_name = 'bdmprojectaccount'\n",
        "storage_container_name = 'bdmcontainerp1'\n",
        "os.environ[\"SPARK_HOME\"] = \"/Users/sebastianneri/spark/spark-3.5.3-bin-hadoop3\"\n",
        "JDBC_JAR = \"/Users/sebastianneri/drivers/postgresql-42.3.6.jar\"\n",
        "POSTGRES_URL = \"postgresql://sebas:mypassword@localhost:5432/exploitation_zone\" # Url of postgres\n",
        "POSTGRES_DRIVER_URL = \"jdbc:postgresql://localhost:5432/exploitation_zone\" # Driver to connect Spark with Postgres\n",
        "os.environ[\"SPARK_LOCAL_IP\"] = \"192.168.1.36\"\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BDMProject\") \\\n",
        "    .master(\"spark://localhost:7077\")\\\n",
        "    .config(\"spark.jars\", JDBC_JAR) \\\n",
        "    .config(\"spark.driver.extraClassPath\", JDBC_JAR) \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZQOLKA6O788v"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/28 21:19:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "connection_string = os.getenv('AZURE_CONNECTION_STRING')\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BDMProject\") \\\n",
        "    .master(\"spark://localhost:7077\")\\\n",
        "    .config(\"spark.jars\", JDBC_JAR) \\\n",
        "    .config(\"spark.driver.extraClassPath\", JDBC_JAR) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"OFF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHQMnHfmq29F"
      },
      "source": [
        "# Transportation\n",
        "\n",
        "Find out more in: https://openrouteservice.org/dev/#/api-docs/optimization/post\n",
        "Alternatives: https://github.com/graphhopper/graphhopper/blob/master/README.md#Map-Matching\n",
        "https://github.com/VROOM-Project/vroom/blob/master/docs/API.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from hdfs import InsecureClient\n",
        "\n",
        "def test_spark_connection() -> bool:\n",
        "    \"\"\"\n",
        "    Verifies the Spark session is active by performing a simple operation.\n",
        "    Returns True if the session is responsive, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Simple operation: create a small DataFrame and collect\n",
        "        count = spark.range(0, 1).count()\n",
        "        print (count == 1)\n",
        "    except Exception:\n",
        "        return print(False)\n",
        "\n",
        "# Run test\n",
        "if __name__ == '__main__':\n",
        "    test_spark_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_struct(flat_schema: dict) -> StructType:\n",
        "    fields = []\n",
        "    for name, typ in flat_schema.items():\n",
        "        if isinstance(typ, list):\n",
        "            elem = typ[0]\n",
        "            if isinstance(elem, dict):\n",
        "                struct = build_struct(elem)\n",
        "                fields.append(StructField(name, ArrayType(struct), True))\n",
        "            else:\n",
        "                spark_type = {'int': IntegerType(), 'float': DoubleType(), 'str': StringType(), 'bool': BooleanType()}.get(elem, StringType())\n",
        "                fields.append(StructField(name, ArrayType(spark_type), True))\n",
        "        else:\n",
        "            spark_type = {'int': IntegerType(), 'float': DoubleType(), 'str': StringType(), 'bool': BooleanType()}.get(typ, StringType())\n",
        "            fields.append(StructField(name, spark_type, True))\n",
        "    return StructType(fields)\n",
        "\n",
        "def string_to_sha256(text: str) -> str:\n",
        "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
        "\n",
        "def flatten_df(df):\n",
        "    from pyspark.sql.types import StructType\n",
        "    flat_cols = []\n",
        "    nested_cols = []\n",
        "    for field in df.schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            nested_cols.append(field.name)\n",
        "        else:\n",
        "            flat_cols.append(col(field.name))\n",
        "    for nested in nested_cols:\n",
        "        for f in df.schema[nested].dataType.fields:\n",
        "            flat_cols.append(col(f\"{nested}.{f.name}\").alias(f\"{nested}_{f.name}\"))\n",
        "    return df.select(flat_cols)\n",
        "\n",
        "# Configuración del Data Lake\n",
        "# Asume que 'file_system_name' es el filesystem de Delta Lake\n",
        "def get_data_lake_service(account_url: str, credential) -> DataLakeServiceClient:\n",
        "    return DataLakeServiceClient(account_url=account_url, credential=credential)\n",
        "\n",
        "@udf(StringType())\n",
        "def sha256_udf(url: str) -> str:\n",
        "    return hashlib.sha256(url.encode('utf-8')).hexdigest() if url else None\n",
        "\n",
        "@udf(StringType())\n",
        "def standardized_hours_udf(ts: str) -> str:\n",
        "    return datetime.fromisoformat(ts).strftime('%H:%M') if ts else None\n",
        "\n",
        "\n",
        "def file_exists(fs_client, path: str) -> bool:\n",
        "    try:\n",
        "        file_client = fs_client.get_file_client(path)\n",
        "        return file_client.exists()\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def upload_file(fs_client, path: str, data: bytes, overwrite=True) -> None:\n",
        "    file_client = fs_client.get_file_client(path)\n",
        "    if overwrite and file_client.exists():\n",
        "        file_client.delete_file()\n",
        "    file_client.create_file()\n",
        "    file_client.append_data(data, offset=0)\n",
        "    file_client.flush_data(len(data))\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# FLATTEN, CAST, SCHEMA Y COMPRESIÓN DE IMÁGENES (sin cambios)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def load_json_schema(path: str) -> dict:\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def compress_image(image_bytes: bytes, max_width: int = 1024, quality: int = 75) -> bytes:\n",
        "    with Image.open(BytesIO(image_bytes)) as img:\n",
        "        if img.mode in (\"RGBA\", \"P\"):\n",
        "            img = img.convert(\"RGB\")\n",
        "        if img.width > max_width:\n",
        "            ratio = max_width / float(img.width)\n",
        "            new_height = int(img.height * ratio)\n",
        "            img = img.resize((max_width, new_height), Image.LANCZOS)\n",
        "        buffer = BytesIO()\n",
        "        img.save(buffer, format=\"JPEG\", quality=quality, optimize=True)\n",
        "        return buffer.getvalue()\n",
        "\n",
        "def process_accommodation_record(record: dict, schema: dict) -> dict:\n",
        "    # Crear DF inferido\n",
        "    accommodation_schema_dict = json.loads(open('accommodation_schema.json').read())\n",
        "    accommodation_struct = build_struct(accommodation_schema_dict)\n",
        "     # Crear DF completo e inferido\n",
        "    df = spark.read.json(spark.sparkContext.parallelize([json.dumps(record)]))\n",
        "\n",
        "    # Extraer columnas planas y hasta 3 niveles de anidación\n",
        "\n",
        "    all_cols = []\n",
        "    for field in df.schema.fields:\n",
        "        if isinstance(field.dataType, StructType):\n",
        "            for subfield in field.dataType.fields:\n",
        "                if isinstance(subfield.dataType, StructType):\n",
        "                    for subsubfield in subfield.dataType.fields:\n",
        "                        if isinstance(subsubfield.dataType, StructType):\n",
        "                            for subsubsubfield in subsubfield.dataType.fields:\n",
        "                                all_cols.append(\n",
        "                                    col(f\"{field.name}.{subfield.name}.{subsubfield.name}.{subsubsubfield.name}\").alias(f\"{field.name}_{subfield.name}_{subsubfield.name}_{subsubsubfield.name}\")\n",
        "                                )\n",
        "                        else:\n",
        "                            all_cols.append(\n",
        "                                col(f\"{field.name}.{subfield.name}.{subsubfield.name}\").alias(f\"{field.name}_{subfield.name}_{subsubfield.name}\")\n",
        "                            )\n",
        "                else:\n",
        "                    all_cols.append(\n",
        "                        col(f\"{field.name}.{subfield.name}\").alias(f\"{field.name}_{subfield.name}\")\n",
        "                    )\n",
        "        else:\n",
        "            all_cols.append(col(field.name))\n",
        "\n",
        "    df_flat = df.select(*all_cols)\n",
        "\n",
        "    # Generar hashes de fotos en Python\n",
        "    photo_urls = record.get(\"property\", {}).get(\"photoUrls\", [])\n",
        "    photo_hashes = [string_to_sha256(u) for u in photo_urls]\n",
        "    df_final = df_flat.withColumn(\"property_photoHash\", lit(photo_hashes))\n",
        "\n",
        "    # Serializar y validar con esquema oficial\n",
        "    json_flat = df_final.toJSON().first()\n",
        "    df_valid = spark.read.schema(accommodation_struct).json(spark.sparkContext.parallelize([json_flat]))\n",
        "    return df_valid.collect()[0].asDict()\n",
        "\n",
        "# --- Procesar registro de clima con Spark flatten y luego aplicar esquema ---\n",
        "def process_weather_record(raw: dict, schema: dict) -> dict:\n",
        "    weather_schema_dict = json.loads(open('weather_schema.json').read())\n",
        "    weather_struct = build_struct(weather_schema_dict)\n",
        "    raw_hourly = raw['hourly']\n",
        "    raw_hourly['timestamp'] = raw_hourly['time']\n",
        "    raw_hourly['time'] = [datetime.fromisoformat(t).strftime('%H:%M') for t in raw_hourly['time']]\n",
        "    \n",
        "    data_rows = [dict(zip(raw_hourly.keys(), values)) for values in zip(*raw_hourly.values())]\n",
        "\n",
        "    # 4. Crear el DataFrame con esquema aplicado\n",
        "    df_valid = spark.createDataFrame(data_rows, schema=weather_struct)\n",
        "    all_rows = [row.asDict() for row in df_valid.collect()]\n",
        "\n",
        "    return all_rows\n",
        "\n",
        "def process_accommodation_images(photo_urls: list, fs_client, city: str) -> None:\n",
        "    for url in photo_urls:\n",
        "        sha = string_to_sha256(url)\n",
        "        path = f\"landing_zone/accommodation_images/{city}/{sha}.jpg\"\n",
        "        trusted_path = f\"trusted_zone/accommodation_images/{city}/{sha}.jpg\"\n",
        "        if file_exists(fs_client, path) and file_exists(fs_client, trusted_path):\n",
        "            continue\n",
        "        if file_exists(fs_client, path):\n",
        "            data = fs_client.get_file_client(path).download_file().readall()\n",
        "        else:\n",
        "            res = req.get(url, stream=True)\n",
        "            try:\n",
        "                res.raise_for_status()\n",
        "            except:\n",
        "                continue\n",
        "            data = res.content\n",
        "            upload_file(fs_client, path, data)\n",
        "        compressed = compress_image(data, max_width=800, quality=70)\n",
        "        upload_file(fs_client, trusted_path, compressed)\n",
        "\n",
        "BACKFILL_LOG = \"/exploitation_zone/backfilled_files.txt\"\n",
        "\n",
        "\n",
        "def image_bytes_to_embedding(\n",
        "    image_bytes: bytes\n",
        ") -> list:\n",
        "\n",
        "    img = Image.open(io.BytesIO(image_bytes)).convert('L')\n",
        "    img = img.resize((32, 32))\n",
        "    arr = np.array(img).astype(np.float32).flatten()\n",
        "    arr /= 255.0\n",
        "    norm = np.linalg.norm(arr)\n",
        "    if norm > 0:\n",
        "        arr /= norm\n",
        "    \n",
        "    return arr\n",
        "\n",
        "\n",
        "\n",
        "def get_or_create_collection(\n",
        "    collection_name: str = \"accommodation_images\",\n",
        "    dim: int = 1024,\n",
        "    index_params: dict = None\n",
        ") -> Collection:\n",
        "\n",
        "    \n",
        "    # 3.2 Define schema if needed\n",
        "    if not utility.has_collection(collection_name):\n",
        "        fields = [\n",
        "            FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n",
        "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
        "        ]\n",
        "        schema = CollectionSchema(fields, description=f\"{collection_name} image embeddings\")\n",
        "        collection = Collection(name=collection_name, schema=schema)\n",
        "        # create index\n",
        "        idx = index_params or {\n",
        "            \"index_type\": \"IVF_FLAT\",\n",
        "            \"params\": {\"nlist\": 128},\n",
        "            \"metric_type\": \"L2\"\n",
        "        }\n",
        "        collection.create_index(field_name=\"embedding\", index_params=idx)\n",
        "    else:\n",
        "        collection = Collection(collection_name)\n",
        "    \n",
        "    # 3.3 Load into memory for speed\n",
        "    collection.load()\n",
        "    return collection\n",
        "\n",
        "\n",
        "def insert_embedding(\n",
        "    collection: Collection,\n",
        "    id: int,\n",
        "    vector: list\n",
        ") -> None:\n",
        "    collection.insert([[id], [vector]])\n",
        "\n",
        "\n",
        "def query_embeddings(\n",
        "    collection: Collection,\n",
        "    expr: str = \"\",\n",
        "    output_fields: list = None,\n",
        "    limit: int = 100\n",
        ") -> list:\n",
        "    output_fields = output_fields or [\"id\", \"embedding\"]\n",
        "    results = collection.query(expr=expr, output_fields=output_fields, limit=limit)\n",
        "    return results\n",
        "\n",
        "def _load_backfill_log() -> set:\n",
        "    \"\"\"\n",
        "    Ensure the backfill_log table exists, then load all file_key values into a set.\n",
        "    \"\"\"\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS backfill_log (\n",
        "                file_key TEXT PRIMARY KEY,\n",
        "                inserted_at TIMESTAMP NOT NULL\n",
        "            )\n",
        "        \"\"\"))\n",
        "        rows = conn.execute(text(\"SELECT file_key FROM backfill_log\")).all()\n",
        "    return {r.file_key for r in rows}\n",
        "\n",
        "\n",
        "\n",
        "def _append_to_backfill_log(log_key: str) -> None:\n",
        "    \"\"\"\n",
        "    Insert a new entry into backfill_log (or do nothing if already present).\n",
        "    \"\"\"\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(\"\"\"\n",
        "            INSERT INTO backfill_log (file_key, inserted_at)\n",
        "            VALUES (:file_key, :inserted_at)\n",
        "            ON CONFLICT (file_key) DO NOTHING\n",
        "        \"\"\"), {\n",
        "            \"file_key\": log_key,\n",
        "            \"inserted_at\": datetime.utcnow()\n",
        "        })\n",
        "\n",
        "def _write_to_postgres(df, table_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Write a Spark DataFrame to Postgres, creating the table if it doesn't exist.\n",
        "    If the table is created, the schema and a sample of the data are shown.\n",
        "    After creation, the function confirms the table exists before writing.\n",
        "    Raises an error if table creation failed.\n",
        "    \"\"\"\n",
        "    inspector = inspect(engine)\n",
        "\n",
        "    if not inspector.has_table(table_name):\n",
        "        print(f\"🛠️ Table '{table_name}' does not exist. Creating it...\")\n",
        "\n",
        "        try:\n",
        "            # Create the table (schema only)\n",
        "            df.limit(0) \\\n",
        "              .write \\\n",
        "              .format(\"jdbc\") \\\n",
        "              .option(\"url\", POSTGRES_DRIVER_URL) \\\n",
        "              .option(\"dbtable\", table_name) \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .save()\n",
        "\n",
        "            # Confirm creation\n",
        "            inspector = inspect(engine)\n",
        "            if not inspector.has_table(table_name):\n",
        "                raise RuntimeError(f\"❌ Failed to create table '{table_name}'.\")\n",
        "\n",
        "            print(\"✅ Table created. Schema:\")\n",
        "            print(\"📊 Sample rows:\")\n",
        "            df.show(5, truncate=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(df.show())\n",
        "            raise RuntimeError(f\"❌ Error creating table '{table_name}': {e}\")\n",
        "\n",
        "    # Append data\n",
        "    try:\n",
        "        df.write \\\n",
        "          .format(\"jdbc\") \\\n",
        "          .option(\"url\", POSTGRES_DRIVER_URL) \\\n",
        "          .option(\"dbtable\", table_name) \\\n",
        "          .mode(\"append\") \\\n",
        "          .save()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(df.show())\n",
        "        raise RuntimeError(f\"❌ Error writing to table '{table_name}': {e}\")\n",
        "\n",
        "\n",
        "def _fetch_files(coll: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Connects to an existing Milvus collection and returns a list of all primary-key IDs.\n",
        "    \"\"\"\n",
        "    # How many entities are stored?\n",
        "    total = coll.num_entities\n",
        "    if total == 0:\n",
        "        return []\n",
        "    # Query for all IDs (no filter, just the 'id' field)\n",
        "    results = coll.query(\n",
        "        expr=\"\", \n",
        "        output_fields=[\"id\"], \n",
        "        limit=total\n",
        "    )\n",
        "    # Extract and return the id values\n",
        "    return [row[\"id\"] for row in results]\n",
        "\n",
        "\n",
        "\n",
        "def get_and_sync_accommodation(\n",
        "    fs,\n",
        "    spark: SparkSession,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    cities: dict,\n",
        "    query_template: dict,\n",
        "    headers:dict,\n",
        "    schema_file: str\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    delta = timedelta(days=1)\n",
        "    log_entries = _load_backfill_log()\n",
        "    coll = get_or_create_collection()\n",
        "    vector_ids = _fetch_files(coll)\n",
        "\n",
        "    for single_date in tqdm([start + i * delta for i in range((end - start).days + 1)]):\n",
        "        arrival = single_date.strftime('%Y-%m-%d')\n",
        "        departure = (single_date + delta).strftime('%Y-%m-%d')\n",
        "        for city, dest_id in tqdm(cities.items()):\n",
        "            landing_path = f\"landing_zone/accommodation/{city}/{arrival}_{departure}.json\"\n",
        "            trusted_path = f\"trusted_zone/accommodation/{city}/{arrival}_{departure}.json\"\n",
        "            \n",
        "            log_key = f\"accommodation/{city}/{arrival}_{departure}.json\"\n",
        "            # filtro: si existe en landing y trusted, nada que hacer\n",
        "            if file_exists(fs, landing_path) and file_exists(fs, trusted_path) and log_key in log_entries and False:\n",
        "                continue\n",
        "\n",
        "            # obtener JSON: de landing o API\n",
        "            if file_exists(fs, landing_path):\n",
        "                raw = fs.get_file_client(landing_path).download_file().readall().decode('utf-8')\n",
        "                data = json.loads(raw)\n",
        "            else:\n",
        "                params = dict(query_template, dest_id=dest_id, arrival_date=arrival, departure_date=departure)\n",
        "                res = req.get(accommodation_endpoint, params=params, headers=headers)\n",
        "                time.sleep(10)\n",
        "                res.raise_for_status()\n",
        "                data = res.json()\n",
        "                upload_file(fs, landing_path, json.dumps(data).encode('utf-8'))\n",
        "            \n",
        "\n",
        "            if file_exists(fs, trusted_path):\n",
        "                trusted = fs.get_file_client(trusted_path).download_file().readall().decode('utf-8')\n",
        "                docs = json.loads(trusted)\n",
        "                photo_hashes = [doc['property_photoHash'] for doc in docs]\n",
        "                photo_urls = [u for h in data['data']['hotels'] for u in h['property']['photoUrls']]\n",
        "\n",
        "            else:\n",
        "                photo_urls = [u for h in data['data']['hotels'] for u in h['property']['photoUrls']]\n",
        "                process_accommodation_images(photo_urls, fs, city)\n",
        "                docs = [process_accommodation_record(r, schema) for r in data['data']['hotels']]\n",
        "                photo_hashes = [doc['property_photoHash'] for doc in docs]\n",
        "                upload_file(fs, trusted_path, json.dumps(docs).encode('utf-8'))\n",
        "            \n",
        "            for hashes, urls in zip(photo_hashes, photo_urls):\n",
        "                    for hash, url in zip(hashes, urls):\n",
        "                        img_path = f\"trusted_zone/accommodation_images/{city}/{hash}.jpg\"\n",
        "                        if not file_exists(fs, img_path):\n",
        "                            process_accommodation_images([url], fs, city)\n",
        "                            data_bytes = fs.get_file_client(img_path).download_file().readall()\n",
        "                            with engine.begin() as conn:\n",
        "                                conn.execute(text(\"\"\"\n",
        "                                    INSERT INTO accommodation_images (id, image_bytes)\n",
        "                                    VALUES (:id, :img)\n",
        "                                    ON CONFLICT (id) DO NOTHING\n",
        "                                \"\"\"), {\"id\": hash, \"img\": data_bytes})\n",
        "                        \n",
        "                        if hash not in vector_ids:\n",
        "                            data_bytes = fs.get_file_client(img_path).download_file().readall()\n",
        "                            emb = image_bytes_to_embedding(data_bytes)\n",
        "                            coll = get_or_create_collection(collection_name=\"accommodation_images\", dim=len(emb))\n",
        "                            insert_embedding(coll, id=hash, vector=emb)\n",
        "                            _append_to_backfill_log(img_path)\n",
        "\n",
        "            exploitation_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(d) for d in docs]))\n",
        "            exploitation_df = exploitation_df.drop('property_priceBreakdown_benefitBadges')\n",
        "            exploitation_df = exploitation_df.withColumn(\"property_priceBreakdown_excludedPrice_value\",exploitation_df[\"property_priceBreakdown_excludedPrice_value\"].cast(DoubleType()))\n",
        "            exploitation_df = exploitation_df.withColumn(\"city_code_name\", lit(city))\n",
        "            _write_to_postgres(exploitation_df, table_name='accommodation')\n",
        "            _append_to_backfill_log(log_key)\n",
        "            \n",
        "\n",
        "            \n",
        "\n",
        "def get_and_sync_weather(\n",
        "    fs,\n",
        "    spark: SparkSession,\n",
        "    start: datetime,\n",
        "    end: datetime,\n",
        "    coords: dict,\n",
        "    query_template: dict,\n",
        "    schema_file: str\n",
        "):\n",
        "    schema = load_json_schema(schema_file)\n",
        "    delta = timedelta(days=1)\n",
        "    log_entries = _load_backfill_log()\n",
        "    \n",
        "    for single_date in tqdm([start + i * delta for i in range((end - start).days + 1)]):\n",
        "        prev_start = (single_date - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "        for city, coord in coords.items():\n",
        "            landing_path = f\"landing_zone/weather/{city}/{prev_start}.json\"\n",
        "            trusted_path = f\"trusted_zone/weather/{city}/{prev_start}.json\"\n",
        "            log_key = f\"weather/{city}/{prev_start}.json\"\n",
        "\n",
        "            # filtro: si existe en landing y trusted, nada que hacer\n",
        "            if file_exists(fs, landing_path) and file_exists(fs, trusted_path) and log_key in log_entries:\n",
        "                continue\n",
        "\n",
        "            # obtener JSON: de landing o API\n",
        "            if file_exists(fs, landing_path):\n",
        "                raw = fs.get_file_client(landing_path).download_file().readall().decode('utf-8')\n",
        "                data = json.loads(raw)\n",
        "            else:\n",
        "                params = dict(query_template, latitude=coord['latitude'], longitude=coord['longitude'], start_date=prev_start, end_date=prev_start)\n",
        "                res = req.get(weather_endpoint, params=params)\n",
        "                time.sleep(10)\n",
        "                res.raise_for_status()\n",
        "                data = res.json()\n",
        "                upload_file(fs, landing_path, json.dumps(data).encode('utf-8'))\n",
        "\n",
        "            if file_exists(fs, trusted_path):\n",
        "                trusted = fs.get_file_client(trusted_path).download_file().readall().decode('utf-8')\n",
        "                data_rows = json.loads(trusted)\n",
        "                weather_schema_dict = json.loads(open('weather_schema.json').read())\n",
        "                weather_struct = build_struct(weather_schema_dict)\n",
        "                df_valid = spark.createDataFrame(data_rows, schema=weather_struct)\n",
        "                doc = [row.asDict() for row in df_valid.collect()]\n",
        "            else:\n",
        "                doc = process_weather_record(data, schema)\n",
        "                upload_file(fs, trusted_path, json.dumps(doc).encode('utf-8'))\n",
        "            \n",
        "\n",
        "            exploitation_df = spark.read.json(spark.sparkContext.parallelize(doc))\n",
        "            exploitation_df = exploitation_df.withColumn(\"city_code_name\", lit(city))\n",
        "            _write_to_postgres(exploitation_df, table_name='weather')\n",
        "            _append_to_backfill_log(log_key)\n",
        "            \n",
        "\n",
        "def list_files(fs_client, prefix: str) -> list:\n",
        "    return [p.name for p in fs_client.get_paths(path=prefix) if not p.is_directory]\n",
        "\n",
        "\n",
        "def find_missing_blobs(fs, landing_prefix: str) -> list:\n",
        "    missing = []\n",
        "    for path in list_files(fs, 'landing_zone/' + landing_prefix):\n",
        "        trusted_path = f\"trusted_zone/{path.replace('landing_zone/', '', 1)}\"\n",
        "        if not file_exists(fs, trusted_path):\n",
        "            missing.append(path)\n",
        "    return missing\n",
        "\n",
        "\n",
        "def backfill_trusted_zone(\n",
        "    fs,\n",
        "    landing_prefix: str,\n",
        "    schema_file: str = None\n",
        ") -> None:\n",
        "    schema = load_json_schema(schema_file) if schema_file else None\n",
        "    to_fill = find_missing_blobs(fs, landing_prefix)\n",
        "\n",
        "    for landing_path in tqdm(to_fill):\n",
        "        trusted_path = f\"trusted_zone/{landing_path.replace('landing_zone/', '', 1)}\"\n",
        "        data_bytes = fs.get_file_client(landing_path).download_file().readall()\n",
        "\n",
        "        # imágenes de alojamiento\n",
        "        if landing_path.startswith('landing_zone/accommodation_images/'):\n",
        "            compressed = compress_image(data_bytes, max_width=800, quality=70)\n",
        "            upload_file(fs, trusted_path, compressed)\n",
        "            continue\n",
        "\n",
        "        # JSON de alojamiento\n",
        "        if landing_path.startswith('landing_zone/accommodation/'):\n",
        "            data = json.loads(data_bytes.decode('utf-8'))\n",
        "            docs = [process_accommodation_record(r, schema) for r in data['data']['hotels']]\n",
        "            upload_file(fs, trusted_path, json.dumps(docs).encode('utf-8'))\n",
        "            continue\n",
        "\n",
        "        # JSON de clima\n",
        "        if landing_path.startswith('landing_zone/weather/'):\n",
        "            data = json.loads(data_bytes.decode('utf-8'))\n",
        "            doc = process_weather_record(data, schema)\n",
        "            upload_file(fs, trusted_path, json.dumps(doc).encode('utf-8'))\n",
        "\n",
        "def backfill_exploitation_zone(\n",
        "    dl_client,\n",
        "    spark: SparkSession,\n",
        "    data_type: str,\n",
        "):\n",
        "    # Load log entries once\n",
        "    log_entries = _load_backfill_log()\n",
        "    coll = get_or_create_collection()\n",
        "    vector_ids = _fetch_files(coll)\n",
        "    # List all files under trusted_zone\n",
        "    all_paths = list_files(dl_client, \"trusted_zone/\")\n",
        "    # Filter relevant paths\n",
        "    qualified = []  # tuples of (full_path, city)\n",
        "    for path in all_paths:\n",
        "        parts = path.split('/')\n",
        "        d_type, city = parts[-3], parts[-2]\n",
        "        if d_type != data_type:\n",
        "            continue\n",
        "        full_path = f\"trusted_zone/{data_type}/{city}/{parts[-1]}\"\n",
        "        qualified.append((full_path, city))\n",
        "\n",
        "    if data_type in (\"accommodation\", \"weather\"):\n",
        "        for full_path, city in tqdm(qualified):\n",
        "            file_name = full_path.split('/')[-1]\n",
        "            log_key = f\"{data_type}/{city}/{file_name}\"\n",
        "            if log_key in log_entries:\n",
        "                continue\n",
        "\n",
        "            # read JSON from trusted\n",
        "            data_bytes = dl_client.get_file_client(full_path).download_file().readall()\n",
        "            docs = json.loads(data_bytes.decode('utf-8'))\n",
        "\n",
        "            # build an RDD of JSON strings (list for accommodation, single dict for weather)\n",
        "            if isinstance(docs, list):\n",
        "                rdd = spark.sparkContext.parallelize([json.dumps(d) for d in docs])\n",
        "            else:\n",
        "                rdd = spark.sparkContext.parallelize([json.dumps(docs)])\n",
        "\n",
        "            exploitation_df = spark.read.json(rdd)\n",
        "            if data_type == 'accommodation':\n",
        "                exploitation_df = exploitation_df.drop('property_priceBreakdown_benefitBadges')\n",
        "                exploitation_df = exploitation_df.withColumn(\"property_priceBreakdown_excludedPrice_value\",exploitation_df[\"property_priceBreakdown_excludedPrice_value\"].cast(DoubleType()))\n",
        "            exploitation_df = exploitation_df.withColumn(\"city_code_name\", lit(city))\n",
        "            _write_to_postgres(exploitation_df, table_name=data_type)\n",
        "            _append_to_backfill_log(log_key)\n",
        "\n",
        "\n",
        "    elif data_type == \"accommodation_images\":\n",
        "        # ensure images table\n",
        "        with engine.begin() as conn:\n",
        "            conn.execute(text(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS accommodation_images (\n",
        "                    id TEXT PRIMARY KEY,\n",
        "                    image_bytes BYTEA NOT NULL\n",
        "                )\n",
        "            \"\"\"))\n",
        "\n",
        "        for full_path, city in tqdm(qualified):\n",
        "            file_name = full_path.split('/')[-1]\n",
        "            image_id = os.path.splitext(file_name)[0]\n",
        "            log_key = f\"{data_type}/{city}/{file_name}\"\n",
        "            \n",
        "            if log_key not in log_entries:\n",
        "                data_bytes = dl_client.get_file_client(full_path).download_file().readall()\n",
        "                with engine.begin() as conn:\n",
        "                    conn.execute(text(\"\"\"\n",
        "                        INSERT INTO accommodation_images (id, image_bytes)\n",
        "                        VALUES (:id, :img)\n",
        "                        ON CONFLICT (id) DO NOTHING\n",
        "                    \"\"\"), {\"id\": image_id, \"img\": data_bytes})\n",
        "            \n",
        "            if file_name not in vector_ids:\n",
        "                data_bytes = dl_client.get_file_client(full_path).download_file().readall()\n",
        "                emb = image_bytes_to_embedding(data_bytes)\n",
        "                coll = get_or_create_collection(collection_name=\"accommodation_images\", dim=len(emb))\n",
        "                insert_embedding(coll, id=image_id, vector=emb)\n",
        "            \n",
        "            _append_to_backfill_log(log_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "destination_ids = {\n",
        "    \"Barcelona\": \"-372490\",\n",
        "    \"Rome\": \"-126693\",\n",
        "    \"Madrid\": \"-390625\",\n",
        "    \"Paris\": \"-1456928\"\n",
        "}\n",
        "\n",
        "destination_coords = {\n",
        "    'Barcelona': {'latitude': 41.3874, 'longitude': 2.1686},\n",
        "    'Paris': {'latitude': 48.8575, 'longitude': 2.3514},\n",
        "    'Madrid': {'latitude': 40.4167, 'longitude': 3.7033},\n",
        "    'Rome': {'latitude': 41.8967, 'longitude': 12.4822}\n",
        "}\n",
        "\n",
        "accommodation_endpoint = \"https://booking-com15.p.rapidapi.com/api/v1/hotels/searchHotels\"\n",
        "weather_endpoint = 'https://archive-api.open-meteo.com/v1/archive'\n",
        "\n",
        "headers = {\n",
        "    \"x-rapidapi-key\": os.environ[\"RAPID_API_KEY\"],\n",
        "    \"x-rapidapi-host\": os.environ[\"RAPID_API_HOST\"]\n",
        "}\n",
        "\n",
        "accommodation_query = {\n",
        "    \"dest_id\": '',\n",
        "    \"search_type\": \"CITY\",\n",
        "    \"arrival_date\": '',\n",
        "    \"departure_date\": '',\n",
        "    \"adults\": \"2\",\n",
        "    \"children_age\": \"0\",\n",
        "    \"room_qty\": \"1\",\n",
        "    \"page_number\": \"1\",\n",
        "    \"units\": \"metric\",\n",
        "    \"temperature_unit\": \"c\",\n",
        "    \"languagecode\": \"en-us\",\n",
        "    \"currency_code\": \"EUR\"\n",
        "}\n",
        "\n",
        "weather_metrics = 'temperature_2m,rain,snowfall,precipitation,cloud_cover,wind_speed_10m,sunshine_duration'\n",
        "\n",
        "weather_query = {\n",
        "    'latitude': '',\n",
        "    'longitude': '',\n",
        "    'hourly': weather_metrics,\n",
        "    'start_date': '',\n",
        "    'end_date': ''\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize DataLakeServiceClient once\n",
        "datalake_client = DataLakeServiceClient.from_connection_string(connection_string)\n",
        "file_system_client = datalake_client.get_file_system_client(storage_container_name)\n",
        "engine = create_engine(POSTGRES_URL)\n",
        "connections.connect(alias=\"default\", host='localhost', port='19530')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings...\n",
            "Inserting embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/_m/dy6l5nmj1h986_4zdymz20240000gn/T/ipykernel_10564/3404640202.py:258: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"inserted_at\": datetime.utcnow()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n",
            "Inserting embeddings...\n",
            "Generating embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4 [00:03<?, ?it/s]\n",
            "  0%|          | 0/2 [00:03<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserting embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m cities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(destination_ids\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# sync on the fly\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m get_and_sync_accommodation(file_system_client, spark, start, end, destination_ids, accommodation_query, headers, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccommodation_schema.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m get_and_sync_weather(file_system_client, spark, start, end, destination_coords, weather_query, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweather_schema.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[17], line 386\u001b[0m, in \u001b[0;36mget_and_sync_accommodation\u001b[0;34m(fs, spark, start, end, cities, query_template, headers, schema_file)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mhash\u001b[39m, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(hashes, urls):\n\u001b[1;32m    385\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrusted_zone/accommodation_images/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mhash\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(fs, img_path):\n\u001b[1;32m    387\u001b[0m         process_accommodation_images([url], fs, city)\n\u001b[1;32m    388\u001b[0m         data_bytes \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mget_file_client(img_path)\u001b[38;5;241m.\u001b[39mdownload_file()\u001b[38;5;241m.\u001b[39mreadall()\n",
            "Cell \u001b[0;32mIn[17], line 51\u001b[0m, in \u001b[0;36mfile_exists\u001b[0;34m(fs_client, path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     file_client \u001b[38;5;241m=\u001b[39m fs_client\u001b[38;5;241m.\u001b[39mget_file_client(path)\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_client\u001b[38;5;241m.\u001b[39mexists()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/tracing/decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/storage/filedatalake/_data_lake_file_client.py:722\u001b[0m, in \u001b[0;36mDataLakeFileClient.exists\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;129m@distributed_trace\u001b[39m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexists\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m    Returns True if a file exists and returns False otherwise.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m    :rtype: bool\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exists(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/storage/filedatalake/_path_client.py:818\u001b[0m, in \u001b[0;36mPathClient._exists\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_exists\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    806\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m    Returns True if a path exists and returns False otherwise.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03m    :rtype: bool\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blob_client\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/tracing/decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/storage/blob/_blob_client.py:989\u001b[0m, in \u001b[0;36mBlobClient.exists\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m version_id \u001b[38;5;241m=\u001b[39m get_version_id(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion_id, kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mblob\u001b[38;5;241m.\u001b[39mget_properties(\n\u001b[1;32m    990\u001b[0m         snapshot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnapshot,\n\u001b[1;32m    991\u001b[0m         version_id\u001b[38;5;241m=\u001b[39mversion_id,\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;66;03m# Encrypted with CPK\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/tracing/decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/storage/blob/_generated/operations/_blob_operations.py:1939\u001b[0m, in \u001b[0;36mBlobOperations.get_properties\u001b[0;34m(self, snapshot, version_id, timeout, request_id_parameter, lease_access_conditions, cpk_info, modified_access_conditions, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m _request\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mformat_url(_request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m   1938\u001b[0m _stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1939\u001b[0m pipeline_response: PipelineResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_pipeline\u001b[38;5;241m.\u001b[39mrun(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m     _request, stream\u001b[38;5;241m=\u001b[39m_stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1941\u001b[0m )\n\u001b[1;32m   1943\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m]:\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:242\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[38;5;241m=\u001b[39m PipelineRequest(request, context)\n\u001b[1;32m    241\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m first_node\u001b[38;5;241m.\u001b[39msend(pipeline_request)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (2 times)]\u001b[0m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/policies/_redirect.py:205\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    203\u001b[0m original_domain \u001b[38;5;241m=\u001b[39m get_domain(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[0;32m--> 205\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m    206\u001b[0m     redirect_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_redirect_location(response)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/storage/filedatalake/_shared/policies.py:527\u001b[0m, in \u001b[0;36mStorageRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retries_remaining:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_retry(response, retry_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m is_checksum_retry(response):\n\u001b[1;32m    529\u001b[0m             retries_remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    530\u001b[0m                 retry_settings,\n\u001b[1;32m    531\u001b[0m                 request\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mhttp_request,\n\u001b[1;32m    532\u001b[0m                 response\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mhttp_response)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (1 times)]\u001b[0m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/storage/filedatalake/_shared/policies.py:301\u001b[0m, in \u001b[0;36mStorageResponseHook.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    296\u001b[0m     upload_stream_current \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupload_stream_current\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    298\u001b[0m response_callback \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_callback\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m    299\u001b[0m     request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_response_hook\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_callback)\n\u001b[0;32m--> 301\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m    303\u001b[0m will_retry \u001b[38;5;241m=\u001b[39m is_retry(response, request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m is_checksum_retry(response)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Auth error could come from Bearer challenge, in which case this request will be made again\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/_base.py:130\u001b[0m, in \u001b[0;36m_TransportRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m:param request: The PipelineRequest object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m:rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m cleanup_kwargs_for_transport(request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PipelineResponse(\n\u001b[1;32m    129\u001b[0m     request\u001b[38;5;241m.\u001b[39mhttp_request,\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sender\u001b[38;5;241m.\u001b[39msend(request\u001b[38;5;241m.\u001b[39mhttp_request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions),\n\u001b[1;32m    131\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mcontext,\n\u001b[1;32m    132\u001b[0m )\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/storage/filedatalake/_shared/base_client.py:353\u001b[0m, in \u001b[0;36mTransportWrapper.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/storage/filedatalake/_shared/base_client.py:353\u001b[0m, in \u001b[0;36mTransportWrapper.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/azure/core/pipeline/transport/_requests_basic.py:363\u001b[0m, in \u001b[0;36mRequestsTransport.send\u001b[0;34m(self, request, proxies, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         read_timeout \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_config\u001b[38;5;241m.\u001b[39mread_timeout)\n\u001b[1;32m    362\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m (connection_timeout, read_timeout)\n\u001b[0;32m--> 363\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    364\u001b[0m         request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    365\u001b[0m         request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    366\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    367\u001b[0m         data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m    368\u001b[0m         files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[1;32m    369\u001b[0m         verify\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection_verify\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_config\u001b[38;5;241m.\u001b[39mverify),\n\u001b[1;32m    370\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    371\u001b[0m         cert\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection_cert\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_config\u001b[38;5;241m.\u001b[39mcert),\n\u001b[1;32m    372\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    373\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    375\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     response\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39menforce_content_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/http/client.py:1423\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/tpc-di/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# parameters\n",
        "start = datetime.strptime('2025-07-01', '%Y-%m-%d')\n",
        "end = datetime.strptime('2025-07-02', '%Y-%m-%d')\n",
        "cities = list(destination_ids.keys())\n",
        "\n",
        "# sync on the fly\n",
        "get_and_sync_accommodation(file_system_client, spark, start, end, destination_ids, accommodation_query, headers, 'accommodation_schema.json')\n",
        "get_and_sync_weather(file_system_client, spark, start, end, destination_coords, weather_query, 'weather_schema.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "backfill_trusted_zone(file_system_client, 'accommodation/', 'accommodation_schema.json')\n",
        "backfill_trusted_zone(file_system_client, 'accommodation_images/')\n",
        "backfill_trusted_zone(file_system_client, 'weather/', 'weather_schema.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_fetch_files(Collection('accommodation_images'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2127 [00:00<?, ?it/s]/var/folders/_m/dy6l5nmj1h986_4zdymz20240000gn/T/ipykernel_18608/1698457556.py:266: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"inserted_at\": datetime.utcnow()\n",
            "100%|██████████| 2127/2127 [09:30<00:00,  3.73it/s]\n"
          ]
        }
      ],
      "source": [
        "backfill_exploitation_zone(file_system_client, spark, 'accommodation')\n",
        "backfill_exploitation_zone(file_system_client, spark, 'accommodation_images')\n",
        "backfill_exploitation_zone(file_system_client, spark, 'weather')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tpc-di",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
